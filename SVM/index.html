<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="SVM"><meta name="keywords" content="Machine Lenrn,SVM"><meta name="author" content="xiaobubuya"><meta name="copyright" content="xiaobubuya"><title>SVM | xiaobubuyaのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"X2MTDP5JXS","apiKey":"b740a5c5a0afcfdb46cd9dde3f3da169","indexName":"xiaobubuya","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '5.3.0'
} </script><meta name="generator" content="Hexo 5.3.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80-%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">一 前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C-%E4%BB%80%E4%B9%88%E6%98%AFSVM%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">二 什么是SVM？</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89-%E7%BA%BF%E6%80%A7SVM"><span class="toc-number">3.</span> <span class="toc-text">三 线性SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1"><span class="toc-number">3.1.</span> <span class="toc-text">1 数学建模</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E2%80%9D%E5%86%B3%E7%AD%96%E9%9D%A2%E2%80%9D%E6%96%B9%E7%A8%8B"><span class="toc-number">3.1.1.</span> <span class="toc-text">（1）”决策面”方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%884%EF%BC%89%E7%BA%BF%E6%80%A7SVM%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E5%9F%BA%E6%9C%AC%E6%8F%8F%E8%BF%B0"><span class="toc-number">3.1.2.</span> <span class="toc-text">（4）线性SVM优化问题基本描述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%886%EF%BC%89%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.3.</span> <span class="toc-text">（6）拉格朗日函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-SMO%E7%AE%97%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">2 SMO算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B-%E7%BC%96%E7%A8%8B%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7SVM"><span class="toc-number">4.</span> <span class="toc-text">四 编程求解线性SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">4.1.</span> <span class="toc-text">（1）可视化数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E7%AE%80%E5%8C%96%E7%89%88SMO%E7%AE%97%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text">（2）简化版SMO算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94-%E6%80%BB%E7%BB%93"><span class="toc-number">5.</span> <span class="toc-text">五 总结</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">xiaobubuya</div><div class="author-info__description text-center">I am Tao Pengyu, who are you</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/xiaobubuya">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">6</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">7</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">5</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img01/%E6%B1%9F%E5%8D%97%E7%83%A7%E9%85%924k%E5%8A%A8%E6%BC%AB%E5%A3%81%E7%BA%B8_%E5%BD%BC%E5%B2%B8%E5%9B%BE%E7%BD%91.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">xiaobubuyaのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">SVM</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-12-31</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Machine-Learn/">Machine Learn</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="一-前言"><a href="#一-前言" class="headerlink" title="一 前言"></a>一 前言</h1><p>说来惭愧，断更快半个月了，本打算是一周一篇的。感觉SVM瞬间难了不少，推导耗费了很多时间，同时身边的事情也不少，忙了许久。本篇文章参考了诸多大牛的文章写成的，对于什么是SVM做出了生动的阐述，同时也进行了线性SVM的理论推导，以及最后的编程实践，公式较多，还需静下心来一点一点推导。</p>
<h1 id="二-什么是SVM？"><a href="#二-什么是SVM？" class="headerlink" title="二 什么是SVM？"></a>二 什么是SVM？</h1><p>SVM的英文全称是Support Vector Machines，我们叫它支持向量机。支持向量机是我们用于分类的一种算法。让我们以一个小故事的形式，开启我们的SVM之旅吧。</p>
<p>在很久以前的情人节，一位大侠要去救他的爱人，但天空中的魔鬼和他玩了一个游戏。</p>
<p>魔鬼在桌子上似乎有规律放了两种颜色的球，说：“你用一根棍分开它们？要求：尽量在放更多球之后，仍然适用。”</p>
<p><img src="https://img-blog.csdn.net/20170923165027315?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>于是大侠这样放，干的不错？</p>
<p><img src="https://img-blog.csdn.net/20170923165115473?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>然后魔鬼，又在桌上放了更多的球，似乎有一个球站错了阵营。显然，大侠需要对棍做出调整。</p>
<p><img src="https://img-blog.csdn.net/20170923165142381?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p><strong>SVM就是试图把棍放在最佳位置，好让在棍的两边有尽可能大的间隙。这个间隙就是球到棍的距离。</strong></p>
<p><img src="https://img-blog.csdn.net/20170923170200381?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>现在好了，即使魔鬼放了更多的球，棍仍然是一个好的分界线。</p>
<p><img src="https://img-blog.csdn.net/20170923170228849?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>魔鬼看到大侠已经学会了一个trick(方法、招式)，于是魔鬼给了大侠一个新的挑战。</p>
<p><img src="https://img-blog.csdn.net/20170923170253159?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>现在，大侠没有棍可以很好帮他分开两种球了，现在怎么办呢？当然像所有武侠片中一样大侠桌子一拍，球飞到空中。然后，凭借大侠的轻功，大侠抓起一张纸，插到了两种球的中间。</p>
<p><img src="https://img-blog.csdn.net/20170923170319898?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>现在，从空中的魔鬼的角度看这些球，这些球看起来像是被一条曲线分开了。</p>
<p><img src="https://img-blog.csdn.net/20170923170416057?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>再之后，无聊的大人们，把这些球叫做<strong>data</strong>，把棍子叫做<strong>classifier</strong>, 找到最大间隙的<strong>trick</strong>叫做<strong>optimization</strong>，拍桌子叫做<strong>kernelling</strong>, 那张纸叫做<strong>hyperplane</strong>。</p>
<p>更为直观地感受一下吧(需要翻墙)：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=3liCbRZPrZA">https://www.youtube.com/watch?v=3liCbRZPrZA</a></p>
<p><strong>概述一下：</strong></p>
<p>当一个分类问题，数据是线性可分的，也就是用一根棍就可以将两种小球分开的时候，我们只要将棍的位置放在让小球距离棍的距离最大化的位置即可，寻找这个最大间隔的过程，就叫做最优化。但是，现实往往是很残酷的，一般的数据是线性不可分的，也就是找不到一个棍将两种小球很好的分类。这个时候，我们就需要像大侠一样，将小球拍起，用一张纸代替小棍将小球进行分类。想要让数据飞起，我们需要的东西就是核函数(kernel)，用于切分小球的纸，就是超平面。</p>
<p>也许这个时候，你还是似懂非懂，没关系。根据刚才的描述，可以看出，问题是从线性可分延伸到线性不可分的。那么，我们就按照这个思路，进行原理性的剖析。</p>
<h1 id="三-线性SVM"><a href="#三-线性SVM" class="headerlink" title="三 线性SVM"></a>三 线性SVM</h1><p>先看下线性可分的二分类问题。</p>
<p><img src="https://img-blog.csdn.net/20170923170544874?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>上图中的(a)是已有的数据，红色和蓝色分别代表两个不同的类别。数据显然是线性可分的，但是将两类数据点分开的直线显然不止一条。上图的(b)和©分别给出了B、C两种不同的分类方案，其中黑色实线为分界线，术语称为“决策面”。每个决策面对应了一个线性分类器。虽然从分类结果上看，分类器A和分类器B的效果是相同的。但是他们的性能是有差距的，看下图：</p>
<p><img src="https://img-blog.csdn.net/20170923170619352?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>在”决策面”不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的”决策面”放置的位置优于分类器C的”决策面”放置的位置，SVM算法也是这么认为的，它的依据就是分类器B的分类间隔比分类器C的分类间隔大。这里涉及到第一个SVM独有的概念”分类间隔”。在保证决策面方向不变且不会出现错分样本的情况下移动决策面，会在原来的决策面两侧找到两个极限位置（越过该位置就会产生错分现象），如虚线所示。虚线的位置由决策面的方向和距离原决策面最近的几个样本的位置决定。而这两条平行虚线正中间的分界线就是在保持当前决策面方向不变的前提下的最优决策面。两条虚线之间的垂直距离就是这个最优决策面对应的分类间隔。显然每一个可能把数据集正确分开的方向都有一个最优决策面（有些方向无论如何移动决策面的位置也不可能将两类样本完全分开），而不同方向的最优决策面的分类间隔通常是不同的，那个具有“最大间隔”的决策面就是SVM要寻找的最优解。而这个真正的最优解对应的两侧虚线所穿过的样本点，就是SVM中的支持样本点，称为”支持向量”。</p>
<h2 id="1-数学建模"><a href="#1-数学建模" class="headerlink" title="1 数学建模"></a>1 数学建模</h2><p>求解这个”决策面”的过程，就是最优化。一个最优化问题通常有两个基本的因素：1）目标函数，也就是你希望什么东西的什么指标达到最好；2）优化对象，你期望通过改变哪些因素来使你的目标函数达到最优。在线性SVM算法中，目标函数显然就是那个”分类间隔”，而优化对象则是决策面。所以要对SVM问题进行数学建模，首先要对上述两个对象（“分类间隔”和”决策面”）进行数学描述。按照一般的思维习惯，我们先描述决策面。</p>
<p>数学建模的时候，先在二维空间建模，然后再推广到多维。</p>
<h3 id="（1）”决策面”方程"><a href="#（1）”决策面”方程" class="headerlink" title="（1）”决策面”方程"></a>（1）”决策面”方程</h3><p>我们都知道二维空间下一条直线的方式如下所示：</p>
<p><img src="https://img-blog.csdn.net/20170923170727730?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>现在我们做个小小的改变，让原来的x轴变成x1，y轴变成x2。</p>
<p><img src="https://img-blog.csdn.net/20170923170755765?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>移项得：</p>
<p><img src="https://img-blog.csdn.net/20170923170822756?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>将公式向量化得：</p>
<p><img src="https://img-blog.csdn.net/20170923170855067?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>进一步向量化，用w列向量和x列向量和标量γ进一步向量化：</p>
<p><img src="https://img-blog.csdn.net/20170923170921357?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>其中，向量w和x分别为：</p>
<p><img src="https://img-blog.csdn.net/20170923170949079?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>这里w1=a，w2=-1。我们都知道，最初的那个直线方程a和b的几何意义，a表示直线的斜率，b表示截距，a决定了直线与x轴正方向的夹角，b决定了直线与y轴交点位置。那么向量化后的直线的w和r的几何意义是什么呢？</p>
<p>现在假设：</p>
<p><img src="https://img-blog.csdn.net/20171009144156759?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>可得：</p>
<p><img src="https://img-blog.csdn.net/20170923171248966?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>在坐标轴上画出直线和向量w：</p>
<p><img src="https://img-blog.csdn.net/20170925193308065?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>蓝色的线代表向量w，红色的先代表直线y。我们可以看到向量w和直线的关系为垂直关系。这说明了向量w也控制这直线的方向，只不过是与这个直线的方向是垂直的。标量γ的作用也没有变，依然决定了直线的截距。此时，我们称w为直线的法向量。</p>
<p>二维空间的直线方程已经推导完成，将其推广到n为空间，就变成了超平面方程。(一个超平面，在二维空间的例子就是一个直线)但是它的公式没变，依然是：</p>
<p><img src="https://img-blog.csdn.net/20170923171351900?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>不同之处在于：</p>
<p><img src="https://img-blog.csdn.net/20170923171420742?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我们已经顺利推导出了”决策面”方程，它就是我们的超平面方程，之后，我们统称其为超平面方程。</p>
<p>###（2）”分类间隔”方程</p>
<p>现在，我们依然对于一个二维平面的简单例子进行推导。</p>
<p><img src="https://img-blog.csdn.net/20170923171540791?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我们已经知道间隔的大小实际上就是支持向量对应的样本点到决策面的距离的二倍。那么图中的距离d我们怎么求？我们高中都学过，点到直线的距离距离公式如下：</p>
<p><img src="https://img-blog.csdn.net/20170923171610356?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>公式中的直线方程为Ax0+By0+C=0，点P的坐标为(x0,y0)。</p>
<p>现在，将直线方程扩展到多维，求得我们现在的超平面方程，对公式进行如下变形：</p>
<p><img src="https://img-blog.csdn.net/20170923171637926?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>这个d就是”分类间隔”。其中||w||表示w的二范数，求所有元素的平方和，然后再开方。比如对于二维平面：</p>
<p><img src="https://img-blog.csdn.net/20170923171706926?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>那么，</p>
<p><img src="https://img-blog.csdn.net/20170923171733417?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我们目的是为了找出一个分类效果好的超平面作为分类器。分类器的好坏的评定依据是分类间隔W=2d的大小，即分类间隔W越大，我们认为这个超平面的分类效果越好。此时，求解超平面的问题就变成了求解分类间隔W最大化的为题。W的最大化也就是d最大化的。</p>
<p>###（3）约束条件</p>
<p>看起来，我们已经顺利获得了目标函数的数学形式。但是为了求解w的最大值。我们不得不面对如下问题：</p>
<ul>
<li>我们如何判断超平面是否将样本点正确分类？</li>
<li>我们知道相求距离d的最大值，我们首先需要找到支持向量上的点，怎么在众多的点中选出支持向量上的点呢？</li>
</ul>
<p>上述我们需要面对的问题就是约束条件，也就是说我们优化的变量d的取值范围受到了限制和约束。事实上约束条件一直是最优化问题里最让人头疼的东西。但既然我们已经知道了这些约束条件确实存在，就不得不用数学语言对他们进行描述。但SVM算法通过一些巧妙的小技巧，将这些约束条件融合到一个不等式里面。</p>
<p>这个二维平面上有两种点，我们分别对它们进行标记：</p>
<ul>
<li>红颜色的圆点标记为1，我们人为规定其为正样本；</li>
<li>蓝颜色的五角星标记为-1，我们人为规定其为负样本。</li>
</ul>
<p>对每个样本点xi加上一个类别标签yi：</p>
<p><img src="https://img-blog.csdn.net/20170923171905897?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>如果我们的超平面方程能够完全正确地对上图的样本点进行分类，就会满足下面的方程：</p>
<p><img src="https://img-blog.csdn.net/20170923171933138?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>如果我们要求再高一点，假设决策面正好处于间隔区域的中轴线上，并且相应的支持向量对应的样本点到决策面的距离为d，那么公式进一步写成：</p>
<p><img src="https://img-blog.csdn.net/20170923172000544?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>上述公式的解释就是，对于所有分类标签为1的样本点，它们到直线的距离都大于等于d(支持向量上的样本点到超平面的距离)。对于所有分类标签为-1的样本点，它们到直线的距离都小于等于d。公式两边都除以d，就可以得到：</p>
<p><img src="https://img-blog.csdn.net/20170923172027169?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>其中，</p>
<p><img src="https://img-blog.csdn.net/20170923172051319?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>因为||w||和d都是标量。所上述公式的两个矢量，依然描述一条直线的法向量和截距。</p>
<p><img src="https://img-blog.csdn.net/20170923172119166?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>上述两个公式，都是描述一条直线，数学模型代表的意义是一样的。现在，让我们对wd和γd重新起个名字，就叫它们w和γ。因此，我们就可以说：“对于存在分类间隔的两类样本点，我们一定可以找到一些超平面面，使其对于所有的样本点均满足下面的条件：”</p>
<p><img src="https://img-blog.csdn.net/20171030230804176?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>上述方程即给出了SVM最优化问题的约束条件。这时候，可能有人会问了，为什么标记为1和-1呢？因为这样标记方便我们将上述方程变成如下形式：</p>
<p><img src="https://img-blog.csdn.net/20170923172259072?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>正是因为标签为1和-1，才方便我们将约束条件变成一个约束方程，从而方便我们的计算。</p>
<h3 id="（4）线性SVM优化问题基本描述"><a href="#（4）线性SVM优化问题基本描述" class="headerlink" title="（4）线性SVM优化问题基本描述"></a>（4）线性SVM优化问题基本描述</h3><p>现在整合一下思路，我们已经得到我们的目标函数：</p>
<p><img src="https://img-blog.csdn.net/20170923172358421?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我们的优化目标是是d最大化。我们已经说过，我们是用支持向量上的样本点求解d的最大化的问题的。那么支持向量上的样本点有什么特点呢？</p>
<p><img src="https://img-blog.csdn.net/20170923172456094?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>你赞同这个观点吗？所有支持向量上的样本点，都满足如上公式。如果不赞同，请重看”分类间隔”方程推导过程。</p>
<p>现在我们就可以将我们的目标函数进一步化简：</p>
<p><img src="https://img-blog.csdn.net/20170923172524509?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>因为，我们只关心支持向量上的点。随后我们求解d的最大化问题变成了||w||的最小化问题。进而||w||的最小化问题等效于</p>
<p><img src="https://img-blog.csdn.net/20170923172617826?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>为什么要做这样的等效呢？这是为了在进行最优化的过程中对目标函数求导时比较方便，但这绝对不影响最优化问题最后的求解。我们将最终的目标函数和约束条件放在一起进行描述：</p>
<p><img src="https://img-blog.csdn.net/20170923172648006?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>这里n是样本点的总个数，缩写s.t.表示”Subject to”，是”服从某某条件”的意思。上述公式描述的是一个典型的不等式约束条件下的二次型函数优化问题，同时也是支持向量机的基本数学模型。</p>
<p>###（5）求解准备</p>
<p>我们已经得到支持向量机的基本数学模型，接下来的问题就是如何根据数学模型，求得我们想要的最优解。在学习求解方法之前，我们得知道一点，想用我下面讲述的求解方法有一个前提，就是我们的目标函数必须是凸函数。理解凸函数，我们还要先明确另一个概念，凸集。在凸几何中，凸集(convex set)是在)凸组合下闭合的放射空间的子集。看一幅图可能更容易理解：</p>
<p><img src="https://img-blog.csdn.net/20170923172757190?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>左右量图都是一个集合。<strong>如果集合中任意2个元素连线上的点也在集合中，那么这个集合就是凸集。</strong>显然，上图中的左图是一个凸集，上图中的右图是一个非凸集。</p>
<p>凸函数的定义也是如此，其几何意义表示为函数任意两点连线上的值大于对应自变量处的函数值。若这里凸集C即某个区间L，那么，设函数f为定义在区间L上的函数，若对L上的任意两点x1，x2和任意的实数λ，λ属于(0,1)，总有：</p>
<p><img src="https://img-blog.csdn.net/20170923172830397?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>则函数f称为L上的凸函数，当且仅当其上镜图（在函数图像上方的点集）为一个凸集。再看一幅图，也许更容易理解：</p>
<p><img src="https://cuijiahua.com/wp-content/uploads/2017/11/ml_8_43_modify.png" alt="img"></p>
<p>像上图这样的函数，它整体就是一个非凸函数，我们无法获得全局最优解的，只能获得局部最优解。比如红框内的部分，如果单独拿出来，它就是一个凸函数。对于我们的目标函数：</p>
<p><img src="https://img-blog.csdn.net/20170923172944919?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>很显然，它是一个凸函数。所以，可以使用我接下来讲述的方法求取最优解。</p>
<p>通常我们需要求解的最优化问题有如下几类：</p>
<ul>
<li>无约束优化问题，可以写为：</li>
</ul>
<p><img src="https://img-blog.csdn.net/20170923173020282?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>- 有等式约束的优化问题，可以写为：</p>
<p><img src="https://img-blog.csdn.net/20170923173050761?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>- 有不等式约束的优化问题，可以写为：</p>
<p><img src="https://img-blog.csdn.net/20170923173128075?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>对于第(a)类的优化问题，尝尝使用的方法就是费马大定理(Fermat)，即使用求取函数f(x)的导数，然后令其为零，可以求得候选最优值，再在这些候选值中验证；如果是凸函数，可以保证是最优解。这也就是我们高中经常使用的求函数的极值的方法。</p>
<p>对于第(b)类的优化问题，常常使用的方法就是拉格朗日乘子法（Lagrange Multiplier) ，即把等式约束h_i(x)用一个系数与f(x)写为一个式子，称为拉格朗日函数，而系数称为拉格朗日乘子。通过拉格朗日函数对各个变量求导，令其为零，可以求得候选值集合，然后验证求得最优值。</p>
<p>对于第©类的优化问题，常常使用的方法就是KKT条件。同样地，我们把所有的等式、不等式约束与f(x)写为一个式子，也叫拉格朗日函数，系数也称拉格朗日乘子，通过一些条件，可以求出最优值的<strong>必要条件</strong>，这个条件称为KKT条件。</p>
<p>必要条件和充要条件如果不理解，可以看下面这句话：</p>
<ul>
<li>A的<strong>必要条件</strong>就是A可以推出的<strong>结论</strong></li>
<li>A的<strong>充分条件</strong>就是可以推出A的<strong>前提</strong></li>
</ul>
<p>了解到这些，现在让我们再看一下我们的最优化问题：</p>
<p><img src="https://img-blog.csdn.net/20170923173238470?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>现在，我们的这个对优化问题属于哪一类？很显然，它属于第©类问题。因为，在学习求解最优化问题之前，我们还要学习两个东西：拉格朗日函数和KKT条件。</p>
<h3 id="（6）拉格朗日函数"><a href="#（6）拉格朗日函数" class="headerlink" title="（6）拉格朗日函数"></a>（6）拉格朗日函数</h3><p>首先，我们先要从宏观的视野上了解一下<strong>拉格朗日对偶问题出现的原因和背景。</strong></p>
<p>我们知道我们要求解的是最小化问题，所以一个直观的想法是如果我能够构造一个函数，使得该函数在可行解区域内与原目标函数完全一致，而在可行解区域外的数值非常大，甚至是无穷大，那么这个<strong>没有约束条件的新目标函数的优化问题就</strong>与原来<strong>有约束条件的原始目标函数的优化问题</strong>是等价的问题。这就是使用拉格朗日方程的目的，它将约束条件放到目标函数中，<strong>从而将有约束优化问题转换为无约束优化问题。</strong></p>
<p>随后，人们又发现，使用拉格朗日获得的函数，使用求导的方法求解依然困难。进而，需要对问题再进行一次转换，即使用一个数学技巧：<strong>拉格朗日对偶。</strong></p>
<p>所以，显而易见的是，我们在拉格朗日优化我们的问题这个道路上，<strong>需要进行下面二个步骤：</strong></p>
<ul>
<li>将有约束的原始目标函数转换为无约束的新构造的拉格朗日目标函数</li>
<li>使用拉格朗日对偶性，将不易求解的优化问题转化为易求解的优化</li>
</ul>
<p>下面，进行第一步：<strong>将有约束的原始目标函数转换为无约束的新构造的拉格朗日目标函数</strong></p>
<p>公式变形如下：</p>
<p><img src="https://img-blog.csdn.net/20170923173537336?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>其中αi是拉格朗日乘子，αi大于等于0，是我们构造新目标函数时引入的系数变量(我们自己设置)。现在我们令：</p>
<p><img src="https://img-blog.csdn.net/20170923173612138?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>当样本点不满足约束条件时，即<strong>在可行解区域外</strong>：</p>
<p><img src="https://img-blog.csdn.net/20170923173641347?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>此时，我们将αi设置为正无穷，此时θ(w)显然也是正无穷。</p>
<p>当样本点满足约束条件时，即<strong>在可行解区域内：</strong></p>
<p><img src="https://img-blog.csdn.net/20170923173710505?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>此时，显然θ(w)为原目标函数本身。我们将上述两种情况结合一下，就得到了新的目标函数：</p>
<p><img src="https://img-blog.csdn.net/20170923173733435?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>此时，再看我们的初衷，就是为了建立一个在可行解区域内与原目标函数相同，在可行解区域外函数值趋近于无穷大的新函数，现在我们做到了。</p>
<p>现在，我们的问题变成了求新目标函数的最小值，即：</p>
<p><img src="https://img-blog.csdn.net/20170923173800720?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>这里用p*表示这个问题的最优值，且和最初的问题是等价的。</p>
<p>接下来，我们进行第二步：<strong>将不易求解的优化问题转化为易求解的优化</strong></p>
<p>我们看一下我们的新目标函数，先求最大值，再求最小值。这样的话，我们首先就要面对带有需要求解的参数w和b的方程，而αi又是不等式约束，这个求解过程不好做。所以，我们需要使用拉格朗日函数对偶性，将最小和最大的位置交换一下，这样就变成了：</p>
<p><img src="https://img-blog.csdn.net/20170923173832514?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>交换以后的新问题是原始问题的对偶问题，这个新问题的最优值用d<em>来表示。而且d</em>&lt;=p*。我们关心的是d=p的时候，这才是我们要的解。需要什么条件才能让d=p呢？</p>
<ul>
<li>首先必须满足这个优化问题是凸优化问题。</li>
<li>其次，需要满足KKT条件。</li>
</ul>
<p>凸优化问题的定义是：<strong>求取最小值的目标函数为凸函数的一类优化问题。</strong>目标函数是凸函数我们已经知道，这个优化问题又是求最小值。所以我们的最优化问题就是凸优化问题。</p>
<p>接下里，就是探讨是否满足KKT条件了。</p>
<p>###（7）KKT条件</p>
<p>我们已经使用拉格朗日函数对我们的目标函数进行了处理，生成了一个新的目标函数。通过一些条件，可以求出最优值的必要条件，这个条件就是接下来要说的KKT条件。一个最优化模型能够表示成下列标准形式：</p>
<p><img src="https://img-blog.csdn.net/20170923174024269?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>KKT条件的全称是Karush-Kuhn-Tucker条件，KKT条件是说最优值条件必须满足以下条件：</p>
<ul>
<li>条件一：经过拉格朗日函数处理之后的新目标函数L(w,b,α)对α求导为零：</li>
<li>条件二：h(x) = 0；</li>
<li>条件三：α*g(x) = 0；</li>
</ul>
<p>对于我们的优化问题：</p>
<p><img src="https://img-blog.csdn.net/20170923174108494?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>显然，条件二已经满足了。另外两个条件为啥也满足呢？</p>
<p>这里原谅我省略一系列证明步骤，感兴趣的可以移步这里：<a target="_blank" rel="noopener" href="http://blog.csdn.net/xianlingmao/article/details/7919597">http://blog.csdn.net/xianlingmao/article/details/7919597</a></p>
<p>这里已经给出了很好的解释。现在，凸优化问题和KKT都满足了，问题转换成了对偶问题。而求解这个对偶学习问题，可以分为三个步骤：首先要让L(w,b,α)关于w和b最小化，然后求对α的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。</p>
<p>###（8）对偶问题求解</p>
<p><strong>第一步：</strong></p>
<p>根据上述推导已知：</p>
<p><img src="https://img-blog.csdn.net/20170923174148322?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>首先固定α，要让L(w,b,α)关于w和b最小化，我们分别对w和b偏导数，令其等于0，即：</p>
<p><img src="https://img-blog.csdn.net/20170923174216260?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>将上述结果带回L(w,b,α)得到：</p>
<p><img src="https://img-blog.csdn.net/20170923174244541?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>从上面的最后一个式子，我们可以看出，此时的L(w,b,α)函数只含有一个变量，即αi。</p>
<p><strong>第二步：</strong></p>
<p>现在内侧的最小值求解完成，我们求解外侧的最大值，从上面的式子得到</p>
<p><img src="https://img-blog.csdn.net/20170923174321119?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>现在我们的优化问题变成了如上的形式。对于这个问题，我们有更高效的优化算法，即序列最小优化（SMO）算法。我们通过这个优化算法能得到α，再根据α，我们就可以求解出w和b，进而求得我们最初的目的：找到超平面，即”决策平面”。</p>
<p><strong>总结一句话：我们为啥使出吃奶的劲儿进行推导？因为我们要将最初的原始问题，转换到可以使用SMO算法求解的问题，这是一种最流行的求解方法。为啥用这种求解方法？因为它牛逼啊！</strong></p>
<h2 id="2-SMO算法"><a href="#2-SMO算法" class="headerlink" title="2 SMO算法"></a>2 SMO算法</h2><p>现在，我们已经得到了可以用SMO算法求解的目标函数，但是对于怎么编程实现SMO算法还是感觉无从下手。那么现在就聊聊如何使用SMO算法进行求解。</p>
<p>###（1）Platt的SMO算法</p>
<p>1996年，John Platt发布了一个称为SMO的强大算法，用于训练SVM。SM表示序列最小化(Sequential Minimal Optimizaion)。Platt的SMO算法是将大优化问题分解为多个小优化问题来求解的。这些小优化问题往往很容易求解，并且对它们进行顺序求解的结果与将它们作为整体来求解的结果完全一致的。在结果完全相同的同时，SMO算法的求解时间短很多。</p>
<p>SMO算法的目标是求出一系列alpha和b，一旦求出了这些alpha，就很容易计算出权重向量w并得到分隔超平面。</p>
<p>SMO算法的工作原理是：每次循环中选择两个alpha进行优化处理。一旦找到了一对合适的alpha，那么就增大其中一个同时减小另一个。这里所谓的”合适”就是指两个alpha必须符合以下两个条件，条件之一就是两个alpha必须要在间隔边界之外，而且第二个条件则是这两个alpha还没有进进行过区间化处理或者不在边界上。</p>
<p>###（2）SMO算法的解法</p>
<p>先来定义特征到结果的输出函数为：</p>
<p><img src="https://img-blog.csdn.net/20170923174435490?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>接着，我们回忆一下原始优化问题，如下：</p>
<p><img src="https://img-blog.csdn.net/20170923174502951?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>求导得：</p>
<p><img src="https://img-blog.csdn.net/20170923174532667?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>将上述公式带入输出函数中：</p>
<p><img src="https://img-blog.csdn.net/20170923174619049?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>与此同时，拉格朗日对偶后得到最终的目标化函数：</p>
<p><img src="https://img-blog.csdn.net/20170923174646051?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>将目标函数变形，在前面增加一个符号，将最大值问题转换成最小值问题：</p>
<p><img src="https://img-blog.csdn.net/20170923174714198?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>实际上，对于上述目标函数，是存在一个假设的，即数据100%线性可分。但是，目前为止，我们知道几乎所有数据都不那么”干净”。这时我们就可以通过引入所谓的<strong>松弛变量</strong>(slack variable)，来允许有些数据点可以处于超平面的错误的一侧。这样我们的优化目标就能保持仍然不变，但是此时我们的约束条件有所改变：</p>
<p><img src="https://img-blog.csdn.net/20170923174748612?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>根据KKT条件可以得出其中αi取值的意义为：</p>
<p><img src="https://img-blog.csdn.net/20170923174820376?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<ul>
<li>对于第1种情况，表明αi是正常分类，在边界内部；</li>
<li>对于第2种情况，表明αi是支持向量，在边界上；</li>
<li>对于第3种情况，表明αi是在两条边界之间。</li>
</ul>
<p>而最优解需要满足KKT条件，即上述3个条件都得满足，以下几种情况出现将会不满足：</p>
<p><img src="https://img-blog.csdn.net/20170923174904018?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>也就是说，如果存在不能满足KKT条件的αi，那么需要更新这些αi，这是第一个约束条件。此外，更新的同时还要受到第二个约束条件的限制，即：</p>
<p><img src="https://img-blog.csdn.net/20170923174937060?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>因为这个条件，我们同时更新两个α值，因为只有成对更新，才能保证更新之后的值仍然满足和为0的约束，假设我们选择的两个乘子为α1和α2：</p>
<p><img src="https://img-blog.csdn.net/20170923175004358?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>其中， ksi为常数。因为两个因子不好同时求解，所以可以先求第二个乘子α2的解（α2 new），得到α2的解（α2 new）之后，再用α2的解（α2 new）表示α1的解（α1 new ）。为了求解α2 new ，得先确定α2 new的取值范围。假设它的上下边界分别为H和L，那么有：</p>
<p><img src="https://img-blog.csdn.net/20170923180329083?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>接下来，综合下面两个条件：</p>
<p><img src="https://img-blog.csdn.net/20170923180545160?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>当y1不等于y2时，即一个为正1，一个为负1的时候，可以得到：</p>
<p><img src="https://img-blog.csdn.net/20170923180611966?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>所以有：</p>
<p><img src="https://img-blog.csdn.net/20170923180638514?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>此时，取值范围如下图所示：</p>
<p><img src="https://img-blog.csdn.net/20170923180705668?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>当y1等于y2时，即两个都为正1或者都为负1，可以得到：</p>
<p><img src="https://img-blog.csdn.net/20170923180937925?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>所以有：</p>
<p><img src="https://img-blog.csdn.net/20170923181001882?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>此时，取值范围如下图所示：</p>
<p><img src="https://img-blog.csdn.net/20170923181027756?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>如此，根据y1和y2异号或同号，可以得出α2 new的上下界分别为：</p>
<p><img src="https://img-blog.csdn.net/20170923181049977?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>这个界限就是编程的时候需要用到的。已经确定了边界，接下来，就是推导迭代式，用于更新 α值。</p>
<p>我们已经知道，更新α的边界，接下来就是讨论如何更新α值。我们依然假设选择的两个乘子为α1和α2。固定这两个乘子，进行推导。于是目标函数变成了：</p>
<p><img src="https://img-blog.csdn.net/20170923181141690?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"> <a target="_blank" rel="noopener" href="https://img-blog.csdn.net/20170923181141690?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">点击放大图片</a></p>
<p>为了描述方便，我们定义如下符号：</p>
<p><img src="https://img-blog.csdn.net/20170923181215838?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>最终目标函数变为：</p>
<p><img src="https://img-blog.csdn.net/20170923181246552?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我们不关心constant的部分，因为对于α1和α2来说，它们都是常数项，在求导的时候，直接变为0。对于这个目标函数，如果对其求导，还有个未知数α1，所以要推导出α1和α2的关系，然后用α2代替α1，这样目标函数就剩一个未知数了，我们就可以求导了，推导出迭代公式。所以现在继续推导α1和α2的关系。注意第一个约束条件：</p>
<p><img src="https://img-blog.csdn.net/20170923181313611?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我们在求α1和α2的时候，可以将α3,α4,…,αn和y3,y4,…,yn看作常数项。因此有：</p>
<p><img src="https://img-blog.csdn.net/20170923181341828?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我们不必关心常数B的大小，现在将上述等式两边同时乘以y1，得到(y1y1=1)：</p>
<p><img src="https://img-blog.csdn.net/20170923181411815?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>其中γ为常数By1，我们不关心这个值，s=y1y2。接下来，我们将得到的α1带入W(α2)公式得：</p>
<p><img src="https://img-blog.csdn.net/20170923181439699?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>这样目标函数中就只剩下α2了，我们对其求偏导（注意：s=y1y2，所以s的平方为1，y1的平方和y2的平方均为1）：</p>
<p><img src="https://img-blog.csdn.net/20170923181503702?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>继续化简，将s=y1y2带入方程。</p>
<p><img src="https://img-blog.csdn.net/20170923181529352?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我们令：</p>
<p><img src="https://img-blog.csdn.net/20170923181558953?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>Ei为误差项，η为学习速率。</p>
<p>再根据我们已知的公式：</p>
<p><img src="https://img-blog.csdn.net/20170923181623658?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>将α2 new继续化简得：</p>
<p><img src="https://img-blog.csdn.net/20170923181649464?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>这样，我们就得到了最终需要的迭代公式。这个是没有经过剪辑是的解，需要考虑约束：</p>
<p><img src="https://img-blog.csdn.net/20170923181716481?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>根据之前推导的α取值范围，我们得到最终的解析解为：</p>
<p><img src="https://img-blog.csdn.net/20170923181742751?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>又因为：</p>
<p><img src="https://img-blog.csdn.net/20170923181810852?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>消去γ得：</p>
<p><img src="https://img-blog.csdn.net/20170923181835098?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>这样，我们就知道了怎样计算α1和α2了，也就是如何对选择的α进行更新。</p>
<p>当我们更新了α1和α2之后，需要重新计算阈值b，因为b关系到了我们f(x)的计算，也就关系到了误差Ei的计算。</p>
<p>我们要根据α的取值范围，去更正b的值，使间隔最大化。当α1 new在0和C之间的时候，根据KKT条件可知，这个点是支持向量上的点。因此，满足下列公式：</p>
<p><img src="https://img-blog.csdn.net/20170923181859506?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>公式两边同时乘以y1得(y1y1=1)：</p>
<p><img src="https://img-blog.csdn.net/20170923181936489?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>因为我们是根据α1和α2的值去更新b，所以单独提出i=1和i=2的时候，整理可得：</p>
<p><img src="https://img-blog.csdn.net/20170923182308642?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>其中前两项为：</p>
<p><img src="https://img-blog.csdn.net/20170923182336636?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>将上述两个公式，整理得：</p>
<p><img src="https://img-blog.csdn.net/20170923182416056?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>同理可得b2 new为：</p>
<p><img src="https://img-blog.csdn.net/20170923182445864?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>当b1和b2都有效的时候，它们是相等的，即：</p>
<p><img src="https://img-blog.csdn.net/20170923182508724?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>当两个乘子都在边界上，则b阈值和KKT条件一致。当不满足的时候，SMO算法选择他们的中点作为新的阈值：</p>
<p><img src="https://img-blog.csdn.net/20170923182533723?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>最后，更新所有的α和b，这样模型就出来了，从而即可求出我们的分类函数。</p>
<p><strong>现在，让我们梳理下SMO算法的步骤：</strong></p>
<ul>
<li>步骤1：计算误差：</li>
</ul>
<p><img src="https://img-blog.csdn.net/20170923182612798?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>- 步骤2：计算上下界L和H：</p>
<p><img src="https://img-blog.csdn.net/20170923182647508?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>- 步骤3：计算η：</p>
<p><img src="https://img-blog.csdn.net/20170923182713063?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>- 步骤4：更新αj：</p>
<p><img src="https://img-blog.csdn.net/20170923182754964?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>- 步骤5：根据取值范围修剪αj：</p>
<p><img src="https://img-blog.csdn.net/20170923182819257?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>- 步骤6：更新αi：</p>
<p><img src="https://img-blog.csdn.net/20170923182850407?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>- 步骤7：更新b1和b2：</p>
<p><img src="https://img-blog.csdn.net/20170923182918560?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>- 步骤8：根据b1和b2更新b：</p>
<p><img src="https://img-blog.csdn.net/20170923182938819?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<h1 id="四-编程求解线性SVM"><a href="#四-编程求解线性SVM" class="headerlink" title="四 编程求解线性SVM"></a>四 编程求解线性SVM</h1><p>已经梳理完了SMO算法实现步骤，接下来按照这个思路编写代码，进行实战练习。</p>
<h2 id="（1）可视化数据集"><a href="#（1）可视化数据集" class="headerlink" title="（1）可视化数据集"></a>（1）可视化数据集</h2><p>我们先使用简单的数据集进行测试，数据集下载地址：<a target="_blank" rel="noopener" href="https://github.com/Jack-Cherish/Machine-Learning/blob/master/SVM/testSet.txt">https://github.com/Jack-Cherish/Machine-Learning/blob/master/SVM/testSet.txt</a></p>
<p>编写程序可视化数据集，看下它是长什么样的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:读取数据</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    fileName - 文件名</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    dataMat - 数据矩阵</span></span><br><span class="line"><span class="string">    labelMat - 数据标签</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-21</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>(<span class="params">fileName</span>):</span></span><br><span class="line">    dataMat = []; labelMat = []</span><br><span class="line">    fr = <span class="built_in">open</span>(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():                                     <span class="comment">#逐行读取，滤除空格等</span></span><br><span class="line">        lineArr = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        dataMat.append([<span class="built_in">float</span>(lineArr[<span class="number">0</span>]), <span class="built_in">float</span>(lineArr[<span class="number">1</span>])])      <span class="comment">#添加数据</span></span><br><span class="line">        labelMat.append(<span class="built_in">float</span>(lineArr[<span class="number">2</span>]))                          <span class="comment">#添加标签</span></span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:数据可视化</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataMat - 数据矩阵</span></span><br><span class="line"><span class="string">    labelMat - 数据标签</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    无</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-21</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showDataSet</span>(<span class="params">dataMat, labelMat</span>):</span></span><br><span class="line">    data_plus = []                                  <span class="comment">#正样本</span></span><br><span class="line">    data_minus = []                                 <span class="comment">#负样本</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataMat)):</span><br><span class="line">        <span class="keyword">if</span> labelMat[i] &gt; <span class="number">0</span>:</span><br><span class="line">            data_plus.append(dataMat[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data_minus.append(dataMat[i])</span><br><span class="line">    data_plus_np = np.array(data_plus)              <span class="comment">#转换为numpy矩阵</span></span><br><span class="line">    data_minus_np = np.array(data_minus)            <span class="comment">#转换为numpy矩阵</span></span><br><span class="line">    plt.scatter(np.transpose(data_plus_np)[<span class="number">0</span>], np.transpose(data_plus_np)[<span class="number">1</span>])   <span class="comment">#正样本散点图</span></span><br><span class="line">    plt.scatter(np.transpose(data_minus_np)[<span class="number">0</span>], np.transpose(data_minus_np)[<span class="number">1</span>]) <span class="comment">#负样本散点图</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    dataMat, labelMat = loadDataSet(<span class="string">&#x27;testSet.txt&#x27;</span>)</span><br><span class="line">    showDataSet(dataMat, labelMat)</span><br><span class="line"><span class="number">12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364</span></span><br></pre></td></tr></table></figure>
<p>运行程序，查看结果：</p>
<p><img src="https://img-blog.csdn.net/20170923183052127?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>这就是我们使用的二维数据集，显然线性可分。现在我们使用简化版的SMO算法进行求解。</p>
<h2 id="（2）简化版SMO算法"><a href="#（2）简化版SMO算法" class="headerlink" title="（2）简化版SMO算法"></a>（2）简化版SMO算法</h2><p>按照上述已经推导的步骤编写代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> types</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:读取数据</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    fileName - 文件名</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    dataMat - 数据矩阵</span></span><br><span class="line"><span class="string">    labelMat - 数据标签</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-21</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>(<span class="params">fileName</span>):</span></span><br><span class="line">    dataMat = []; labelMat = []</span><br><span class="line">    fr = <span class="built_in">open</span>(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():                                     <span class="comment">#逐行读取，滤除空格等</span></span><br><span class="line">        lineArr = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        dataMat.append([<span class="built_in">float</span>(lineArr[<span class="number">0</span>]), <span class="built_in">float</span>(lineArr[<span class="number">1</span>])])      <span class="comment">#添加数据</span></span><br><span class="line">        labelMat.append(<span class="built_in">float</span>(lineArr[<span class="number">2</span>]))                          <span class="comment">#添加标签</span></span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:随机选择alpha</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    i - alpha</span></span><br><span class="line"><span class="string">    m - alpha参数个数</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    j -</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-21</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectJrand</span>(<span class="params">i, m</span>):</span></span><br><span class="line">    j = i                                 <span class="comment">#选择一个不等于i的j</span></span><br><span class="line">    <span class="keyword">while</span> (j == i):</span><br><span class="line">        j = <span class="built_in">int</span>(random.uniform(<span class="number">0</span>, m))</span><br><span class="line">    <span class="keyword">return</span> j</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:修剪alpha</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    aj - alpha值</span></span><br><span class="line"><span class="string">    H - alpha上限</span></span><br><span class="line"><span class="string">    L - alpha下限</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    aj - alpah值</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-21</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clipAlpha</span>(<span class="params">aj,H,L</span>):</span></span><br><span class="line">    <span class="keyword">if</span> aj &gt; H:</span><br><span class="line">        aj = H</span><br><span class="line">    <span class="keyword">if</span> L &gt; aj:</span><br><span class="line">        aj = L</span><br><span class="line">    <span class="keyword">return</span> aj</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:简化版SMO算法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataMatIn - 数据矩阵</span></span><br><span class="line"><span class="string">    classLabels - 数据标签</span></span><br><span class="line"><span class="string">    C - 松弛变量</span></span><br><span class="line"><span class="string">    toler - 容错率</span></span><br><span class="line"><span class="string">    maxIter - 最大迭代次数</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    无</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-23</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smoSimple</span>(<span class="params">dataMatIn, classLabels, C, toler, maxIter</span>):</span></span><br><span class="line">    <span class="comment">#转换为numpy的mat存储</span></span><br><span class="line">    dataMatrix = np.mat(dataMatIn); labelMat = np.mat(classLabels).transpose()</span><br><span class="line">    <span class="comment">#初始化b参数，统计dataMatrix的维度</span></span><br><span class="line">    b = <span class="number">0</span>; m,n = np.shape(dataMatrix)</span><br><span class="line">    <span class="comment">#初始化alpha参数，设为0</span></span><br><span class="line">    alphas = np.mat(np.zeros((m,<span class="number">1</span>)))</span><br><span class="line">    <span class="comment">#初始化迭代次数</span></span><br><span class="line">    iter_num = <span class="number">0</span></span><br><span class="line">    <span class="comment">#最多迭代matIter次</span></span><br><span class="line">    <span class="keyword">while</span> (iter_num &lt; maxIter):</span><br><span class="line">        alphaPairsChanged = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="comment">#步骤1：计算误差Ei</span></span><br><span class="line">            fXi = <span class="built_in">float</span>(np.multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b</span><br><span class="line">            Ei = fXi - <span class="built_in">float</span>(labelMat[i])</span><br><span class="line">            <span class="comment">#优化alpha，更设定一定的容错率。</span></span><br><span class="line">            <span class="keyword">if</span> ((labelMat[i]*Ei &lt; -toler) <span class="keyword">and</span> (alphas[i] &lt; C)) <span class="keyword">or</span> ((labelMat[i]*Ei &gt; toler) <span class="keyword">and</span> (alphas[i] &gt; <span class="number">0</span>)):</span><br><span class="line">                <span class="comment">#随机选择另一个与alpha_i成对优化的alpha_j</span></span><br><span class="line">                j = selectJrand(i,m)</span><br><span class="line">                <span class="comment">#步骤1：计算误差Ej</span></span><br><span class="line">                fXj = <span class="built_in">float</span>(np.multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b</span><br><span class="line">                Ej = fXj - <span class="built_in">float</span>(labelMat[j])</span><br><span class="line">                <span class="comment">#保存更新前的aplpha值，使用深拷贝</span></span><br><span class="line">                alphaIold = alphas[i].copy(); alphaJold = alphas[j].copy();</span><br><span class="line">                <span class="comment">#步骤2：计算上下界L和H</span></span><br><span class="line">                <span class="keyword">if</span> (labelMat[i] != labelMat[j]):</span><br><span class="line">                    L = <span class="built_in">max</span>(<span class="number">0</span>, alphas[j] - alphas[i])</span><br><span class="line">                    H = <span class="built_in">min</span>(C, C + alphas[j] - alphas[i])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    L = <span class="built_in">max</span>(<span class="number">0</span>, alphas[j] + alphas[i] - C)</span><br><span class="line">                    H = <span class="built_in">min</span>(C, alphas[j] + alphas[i])</span><br><span class="line">                <span class="keyword">if</span> L==H: print(<span class="string">&quot;L==H&quot;</span>); <span class="keyword">continue</span></span><br><span class="line">                <span class="comment">#步骤3：计算eta</span></span><br><span class="line">                eta = <span class="number">2.0</span> * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T</span><br><span class="line">                <span class="keyword">if</span> eta &gt;= <span class="number">0</span>: print(<span class="string">&quot;eta&gt;=0&quot;</span>); <span class="keyword">continue</span></span><br><span class="line">                <span class="comment">#步骤4：更新alpha_j</span></span><br><span class="line">                alphas[j] -= labelMat[j]*(Ei - Ej)/eta</span><br><span class="line">                <span class="comment">#步骤5：修剪alpha_j</span></span><br><span class="line">                alphas[j] = clipAlpha(alphas[j],H,L)</span><br><span class="line">                <span class="keyword">if</span> (<span class="built_in">abs</span>(alphas[j] - alphaJold) &lt; <span class="number">0.00001</span>): print(<span class="string">&quot;alpha_j变化太小&quot;</span>); <span class="keyword">continue</span></span><br><span class="line">                <span class="comment">#步骤6：更新alpha_i</span></span><br><span class="line">                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])</span><br><span class="line">                <span class="comment">#步骤7：更新b_1和b_2</span></span><br><span class="line">                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T</span><br><span class="line">                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T</span><br><span class="line">                <span class="comment">#步骤8：根据b_1和b_2更新b</span></span><br><span class="line">                <span class="keyword">if</span> (<span class="number">0</span> &lt; alphas[i]) <span class="keyword">and</span> (C &gt; alphas[i]): b = b1</span><br><span class="line">                <span class="keyword">elif</span> (<span class="number">0</span> &lt; alphas[j]) <span class="keyword">and</span> (C &gt; alphas[j]): b = b2</span><br><span class="line">                <span class="keyword">else</span>: b = (b1 + b2)/<span class="number">2.0</span></span><br><span class="line">                <span class="comment">#统计优化次数</span></span><br><span class="line">                alphaPairsChanged += <span class="number">1</span></span><br><span class="line">                <span class="comment">#打印统计信息</span></span><br><span class="line">                print(<span class="string">&quot;第%d次迭代 样本:%d, alpha优化次数:%d&quot;</span> % (iter_num,i,alphaPairsChanged))</span><br><span class="line">        <span class="comment">#更新迭代次数</span></span><br><span class="line">        <span class="keyword">if</span> (alphaPairsChanged == <span class="number">0</span>): iter_num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>: iter_num = <span class="number">0</span></span><br><span class="line">        print(<span class="string">&quot;迭代次数: %d&quot;</span> % iter_num)</span><br><span class="line">    <span class="keyword">return</span> b,alphas</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:分类结果可视化</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataMat - 数据矩阵</span></span><br><span class="line"><span class="string">    w - 直线法向量</span></span><br><span class="line"><span class="string">    b - 直线解决</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    无</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-23</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showClassifer</span>(<span class="params">dataMat, w, b</span>):</span></span><br><span class="line">    <span class="comment">#绘制样本点</span></span><br><span class="line">    data_plus = []                                  <span class="comment">#正样本</span></span><br><span class="line">    data_minus = []                                 <span class="comment">#负样本</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataMat)):</span><br><span class="line">        <span class="keyword">if</span> labelMat[i] &gt; <span class="number">0</span>:</span><br><span class="line">            data_plus.append(dataMat[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data_minus.append(dataMat[i])</span><br><span class="line">    data_plus_np = np.array(data_plus)              <span class="comment">#转换为numpy矩阵</span></span><br><span class="line">    data_minus_np = np.array(data_minus)            <span class="comment">#转换为numpy矩阵</span></span><br><span class="line">    plt.scatter(np.transpose(data_plus_np)[<span class="number">0</span>], np.transpose(data_plus_np)[<span class="number">1</span>], s=<span class="number">30</span>, alpha=<span class="number">0.7</span>)   <span class="comment">#正样本散点图</span></span><br><span class="line">    plt.scatter(np.transpose(data_minus_np)[<span class="number">0</span>], np.transpose(data_minus_np)[<span class="number">1</span>], s=<span class="number">30</span>, alpha=<span class="number">0.7</span>) <span class="comment">#负样本散点图</span></span><br><span class="line">    <span class="comment">#绘制直线</span></span><br><span class="line">    x1 = <span class="built_in">max</span>(dataMat)[<span class="number">0</span>]</span><br><span class="line">    x2 = <span class="built_in">min</span>(dataMat)[<span class="number">0</span>]</span><br><span class="line">    a1, a2 = w</span><br><span class="line">    b = <span class="built_in">float</span>(b)</span><br><span class="line">    a1 = <span class="built_in">float</span>(a1[<span class="number">0</span>])</span><br><span class="line">    a2 = <span class="built_in">float</span>(a2[<span class="number">0</span>])</span><br><span class="line">    y1, y2 = (-b- a1*x1)/a2, (-b - a1*x2)/a2</span><br><span class="line">    plt.plot([x1, x2], [y1, y2])</span><br><span class="line">    <span class="comment">#找出支持向量点</span></span><br><span class="line">    <span class="keyword">for</span> i, alpha <span class="keyword">in</span> <span class="built_in">enumerate</span>(alphas):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(alpha) &gt; <span class="number">0</span>:</span><br><span class="line">            x, y = dataMat[i]</span><br><span class="line">            plt.scatter([x], [y], s=<span class="number">150</span>, c=<span class="string">&#x27;none&#x27;</span>, alpha=<span class="number">0.7</span>, linewidth=<span class="number">1.5</span>, edgecolor=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:计算w</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataMat - 数据矩阵</span></span><br><span class="line"><span class="string">    labelMat - 数据标签</span></span><br><span class="line"><span class="string">    alphas - alphas值</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    无</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-23</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_w</span>(<span class="params">dataMat, labelMat, alphas</span>):</span></span><br><span class="line">    alphas, dataMat, labelMat = np.array(alphas), np.array(dataMat), np.array(labelMat)</span><br><span class="line">    w = np.dot((np.tile(labelMat.reshape(<span class="number">1</span>, -<span class="number">1</span>).T, (<span class="number">1</span>, <span class="number">2</span>)) * dataMat).T, alphas)</span><br><span class="line">    <span class="keyword">return</span> w.tolist()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    dataMat, labelMat = loadDataSet(<span class="string">&#x27;testSet.txt&#x27;</span>)</span><br><span class="line">    b,alphas = smoSimple(dataMat, labelMat, <span class="number">0.6</span>, <span class="number">0.001</span>, <span class="number">40</span>)</span><br><span class="line">    w = get_w(dataMat, labelMat, alphas)</span><br><span class="line">    showClassifer(dataMat, w, b)</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239</span></span><br></pre></td></tr></table></figure>
<p>程序运行结果：</p>
<p><img src="https://img-blog.csdn.net/20170923183337571?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>其中，中间的蓝线为求出来的分类器，用红圈圈出的点为支持向量点。</p>
<h1 id="五-总结"><a href="#五-总结" class="headerlink" title="五 总结"></a>五 总结</h1><ul>
<li>本文主要进行了线性SVM的推导，并通过编程实现一个简化版SMO算法；</li>
<li>本文的简化版SMO算法在选取α的时候，没有选择启发式的选择方法，并且没有两个乘子的计算没有进行优化，所以算法比较耗时，下一篇文章会讲解相应的优化方法；</li>
<li>本文讨论的是线性SVM，没有使用核函数，下一篇文章将会讲解如何应用核函数，将SVM应用于非线性数据集；</li>
<li>如有问题，请留言。如有错误，还望指正，谢谢！</li>
</ul>
<p><strong>PS： 如果觉得本篇本章对您有所帮助，欢迎关注、评论、赞！</strong></p>
<p><strong>参考资料：</strong></p>
<ul>
<li>[1] 五岁小孩也能看懂的SVM：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/21094489/answer/8627319">https://www.zhihu.com/question/21094489/answer/8627319</a></li>
<li>[2] 五岁小孩也能看懂的SVM ：<a target="_blank" rel="noopener" href="https://www.reddit.com/r/MachineLearning/comments/15zrpp/please_explain_support_vector_machines_svm_like_i/">https://www.reddit.com/r/MachineLearning/comments/15zrpp/please_explain_support_vector_machines_svm_like_i/</a></li>
<li>[3] pluskid大牛博客：<a target="_blank" rel="noopener" href="http://blog.pluskid.org/?page_id=683">http://blog.pluskid.org/?page_id=683</a></li>
<li>[4] 陈东岳老师文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/24638007">https://zhuanlan.zhihu.com/p/24638007</a></li>
<li>[5] 深入理解拉格朗日乘子法和KKT条件：<a target="_blank" rel="noopener" href="http://blog.csdn.net/xianlingmao/article/details/7919597">http://blog.csdn.net/xianlingmao/article/details/7919597</a></li>
<li>[6] 充分条件和必要条件：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/30469121">https://www.zhihu.com/question/30469121</a></li>
<li>[7] 凸函数：<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%87%B8%E5%87%BD%E6%95%B0">https://zh.wikipedia.org/wiki/%E5%87%B8%E5%87%BD%E6%95%B0</a></li>
<li>[8]《机器学习实战》第6章内容。</li>
<li>[9] SVM之SMO算法：<a target="_blank" rel="noopener" href="http://www.cnblogs.com/zangrunqiang/p/5515872.html">http://www.cnblogs.com/zangrunqiang/p/5515872.html</a></li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">xiaobubuya</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://xiaobubuya.github.io/SVM/">https://xiaobubuya.github.io/SVM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Machine-Lenrn/">Machine Lenrn</a><a class="post-meta__tags" href="/tags/SVM/">SVM</a></div><nav id="pagination"><div class="next-post pull-right"><a href="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/"><span>计算机网络面试知识点总结</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img01/%E6%B1%9F%E5%8D%97%E7%83%A7%E9%85%924k%E5%8A%A8%E6%BC%AB%E5%A3%81%E7%BA%B8_%E5%BD%BC%E5%B2%B8%E5%9B%BE%E7%BD%91.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By xiaobubuya</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>