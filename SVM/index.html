<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="SVM"><meta name="keywords" content="SVM"><meta name="author" content="xiaobubuya"><meta name="copyright" content="xiaobubuya"><title>SVM | xiaobubuyaのBlog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"X2MTDP5JXS","apiKey":"b740a5c5a0afcfdb46cd9dde3f3da169","indexName":"xiaobubuya","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '5.3.0'
} </script><meta name="generator" content="Hexo 5.3.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80-%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">一 前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C-%E4%BB%80%E4%B9%88%E6%98%AFSVM%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">二 什么是SVM？</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89-%E7%BA%BF%E6%80%A7SVM"><span class="toc-number">3.</span> <span class="toc-text">三 线性SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1"><span class="toc-number">3.1.</span> <span class="toc-text">1 数学建模</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E2%80%9D%E5%86%B3%E7%AD%96%E9%9D%A2%E2%80%9D%E6%96%B9%E7%A8%8B"><span class="toc-number">3.1.1.</span> <span class="toc-text">（1）”决策面”方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E2%80%9D%E5%88%86%E7%B1%BB%E9%97%B4%E9%9A%94%E2%80%9D%E6%96%B9%E7%A8%8B"><span class="toc-number">3.1.2.</span> <span class="toc-text">（2）”分类间隔”方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E7%BA%A6%E6%9D%9F%E6%9D%A1%E4%BB%B6"><span class="toc-number">3.1.3.</span> <span class="toc-text">（3）约束条件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%884%EF%BC%89%E7%BA%BF%E6%80%A7SVM%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E5%9F%BA%E6%9C%AC%E6%8F%8F%E8%BF%B0"><span class="toc-number">3.1.4.</span> <span class="toc-text">（4）线性SVM优化问题基本描述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%885%EF%BC%89%E6%B1%82%E8%A7%A3%E5%87%86%E5%A4%87"><span class="toc-number">3.1.5.</span> <span class="toc-text">（5）求解准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%886%EF%BC%89%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.6.</span> <span class="toc-text">（6）拉格朗日函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%887%EF%BC%89KKT%E6%9D%A1%E4%BB%B6"><span class="toc-number">3.1.7.</span> <span class="toc-text">（7）KKT条件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%888%EF%BC%89%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A3"><span class="toc-number">3.1.8.</span> <span class="toc-text">（8）对偶问题求解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-SMO%E7%AE%97%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">2 SMO算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ai%E7%9A%84%E5%8F%96%E5%80%BC%E8%8C%83%E5%9B%B4"><span class="toc-number">3.2.0.1.</span> <span class="toc-text">ai的取值范围</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E8%80%83%E9%87%8F%EF%BC%9A"><span class="toc-number">3.2.0.2.</span> <span class="toc-text">参数优化考量：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="toc-number">3.2.0.3.</span> <span class="toc-text">优化方法：</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B-%E7%BC%96%E7%A8%8B%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7SVM"><span class="toc-number">4.</span> <span class="toc-text">四 编程求解线性SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">4.1.</span> <span class="toc-text">（1）可视化数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E7%AE%80%E5%8C%96%E7%89%88SMO%E7%AE%97%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text">（2）简化版SMO算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94-%E6%80%BB%E7%BB%93"><span class="toc-number">5.</span> <span class="toc-text">五 总结</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img02/%E5%A4%B4%E5%83%8F.jpg"></div><div class="author-info__name text-center">xiaobubuya</div><div class="author-info__description text-center">I am Tao Pengyu, who are you</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/xiaobubuya">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">8</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">11</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">7</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://xiaobubuya.cn/gallery">Gallery</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://xiaobubuya.cn/slides">Slides</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img01/%E6%B1%9F%E5%8D%97%E7%83%A7%E9%85%924k%E5%8A%A8%E6%BC%AB%E5%A3%81%E7%BA%B8_%E5%BD%BC%E5%B2%B8%E5%9B%BE%E7%BD%91.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">xiaobubuyaのBlog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">SVM</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-12-31</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Machine-Learn/">Machine Learn</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">8.1k</span><span class="post-meta__separator">|</span><span>Reading time: 28 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="一-前言"><a href="#一-前言" class="headerlink" title="一 前言"></a>一 前言</h1><p>SVM推导过程确实有点难，不是几个小时就能理解透彻的，个人小小记录一下。</p>
<h1 id="二-什么是SVM？"><a href="#二-什么是SVM？" class="headerlink" title="二 什么是SVM？"></a>二 什么是SVM？</h1><p>SVM的英文全称是Support Vector Machines，我们叫它支持向量机。支持向量机是我们用于分类的一种算法。</p>
<p>首先，在桌子上有规律放了两种颜色的球，需要用一根棍分开它们？要求：尽量在放更多球之后，仍然适用。</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923165027315"></p>
<p>于是我们可以这样放：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923165115473"></p>
<p>在刚刚的基础上放了更多的球，似乎有一个球站错了阵营。显然，我们需要对棍做出调整。</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923165142381"></p>
<p><strong>SVM就是试图把棍放在最佳位置，好让在棍的两边有尽可能大的间隙。这个间隙就是球到棍的距离。</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923170200381"></p>
<p>现在好了，即使更多的球，棍仍然是一个好的分界线。</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923170228849"></p>
<p>我们已经学会了一个trick(方法、招式)，但是对于以下的球来说，我们似乎不能完美的将它们分开。</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923170253159"></p>
<p>现在怎么办呢？当然像所有武侠片中一样大侠桌子一拍，球飞到空中。然后，用起一张纸，插到了两种球的中间。</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923170319898"></p>
<p>现在，从空中的角度看这些球，这些球看起来像是被一条曲线分开了。</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923170416057"></p>
<p>再之后，我们就可以把这些球叫做<strong>data</strong>，把棍子叫做<strong>classifier</strong>, 找到最大间隙的<strong>trick</strong>叫做<strong>optimization</strong>，拍桌子叫做<strong>kernelling</strong>, 那张纸叫做<strong>hyperplane</strong>。</p>
<p><strong>概述一下：</strong></p>
<p>当一个分类问题，数据是线性可分的，也就是用一根棍就可以将两种小球分开的时候，我们只要将棍的位置放在让小球距离棍的距离最大化的位置即可，寻找这个最大间隔的过程，就叫做最优化。但是，现实往往是很残酷的，一般的数据是线性不可分的，也就是找不到一个棍将两种小球很好的分类。这个时候，我们就需要将小球拍起，用一张纸代替小棍将小球进行分类。想要让数据飞起，我们需要的东西就是核函数(kernel)，用于切分小球的纸，就是超平面。</p>
<p>也许这个时候，你还是似懂非懂，没关系。根据刚才的描述，可以看出，问题是从线性可分延伸到线性不可分的。那么，我们就按照这个思路，进行原理性的剖析。</p>
<h1 id="三-线性SVM"><a href="#三-线性SVM" class="headerlink" title="三 线性SVM"></a>三 线性SVM</h1><p>先看下线性可分的二分类问题。</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923170544874"></p>
<p>上图中的(a)是已有的数据，红色和蓝色分别代表两个不同的类别。数据显然是线性可分的，但是将两类数据点分开的直线显然不止一条。上图的(b)和©分别给出了B、C两种不同的分类方案，其中黑色实线为分界线，术语称为“决策面”。每个决策面对应了一个线性分类器。虽然从分类结果上看，分类器A和分类器B的效果是相同的。但是他们的性能是有差距的，看下图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923170619352"></p>
<p>在”决策面”不变的情况下，我又添加了一个红点。可以看到，分类器B依然能很好的分类结果，而分类器C则出现了分类错误。显然分类器B的”决策面”放置的位置优于分类器C的”决策面”放置的位置，SVM算法也是这么认为的，它的依据就是分类器B的分类间隔比分类器C的分类间隔大。这里涉及到第一个SVM独有的概念”分类间隔”。在保证决策面方向不变且不会出现错分样本的情况下移动决策面，会在原来的决策面两侧找到两个极限位置（越过该位置就会产生错分现象），如虚线所示。虚线的位置由决策面的方向和距离原决策面最近的几个样本的位置决定。而这两条平行虚线正中间的分界线就是在保持当前决策面方向不变的前提下的最优决策面。两条虚线之间的垂直距离就是这个最优决策面对应的分类间隔。显然每一个可能把数据集正确分开的方向都有一个最优决策面（有些方向无论如何移动决策面的位置也不可能将两类样本完全分开），而不同方向的最优决策面的分类间隔通常是不同的，那个具有“最大间隔”的决策面就是SVM要寻找的最优解。而这个真正的最优解对应的两侧虚线所穿过的样本点，就是SVM中的支持样本点，称为”支持向量”。</p>
<h2 id="1-数学建模"><a href="#1-数学建模" class="headerlink" title="1 数学建模"></a>1 数学建模</h2><p>求解这个”决策面”的过程，就是最优化。一个最优化问题通常有两个基本的因素：1）目标函数，也就是你希望什么东西的什么指标达到最好；2）优化对象，你期望通过改变哪些因素来使你的目标函数达到最优。在线性SVM算法中，目标函数显然就是那个”分类间隔”，而优化对象则是决策面。所以要对SVM问题进行数学建模，首先要对上述两个对象（“分类间隔”和”决策面”）进行数学描述。按照一般的思维习惯，我们先描述决策面。</p>
<p>数学建模的时候，先在二维空间建模，然后再推广到多维。</p>
<h3 id="（1）”决策面”方程"><a href="#（1）”决策面”方程" class="headerlink" title="（1）”决策面”方程"></a>（1）”决策面”方程</h3><p>我们都知道二维空间下一条直线的方式如下所示：</p>
<p>$$<br>y = ax+b<br>$$</p>
<p>现在我们做个小小的改变，让原来的x轴变成x1，y轴变成x2。</p>
<p>$$<br>x_2 = ax_1 +b<br>$$</p>
<p>移项得：</p>
<p>$$<br>ax_1-x_2+b=0<br>$$</p>
<p>将公式向量化得：</p>
<p>$$<br>\left[<br>\matrix{<br>  x_1&amp;-1\<br>}<br>\right]<br>\left[<br>\matrix{<br>  x_1\<br>  x_2<br>}<br>\right]+b=0<br>$$</p>
<p>$$</p>
<p>$$</p>
<p>进一步向量化，用w列向量和x列向量和标量γ进一步向量化：<br>$$<br>w^Tx+\gamma=0<br>$$</p>
<p>其中，向量w和x分别为：</p>
<p>$$<br>w=\left[<br>\matrix{<br>  w_1&amp;w_2\<br>}<br>\right]^T,x=\left[<br>\matrix{<br>  x_1&amp;x_2\<br>}<br>\right]^T<br>$$</p>
<p>这里w1=a，w2=-1。我们都知道，最初的那个直线方程a和b的几何意义，a表示直线的斜率，b表示截距，a决定了直线与x轴正方向的夹角，b决定了直线与y轴交点位置。那么向量化后的直线的w和r的几何意义是什么呢？</p>
<p>现在假设：</p>
<p>$$<br>w=\left[<br>\matrix{<br>  \sqrt{3}&amp;-1\<br>}<br>\right]^T<br>$$</p>
<p>在坐标轴上画出直线和向量w：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170925193308065"></p>
<p>蓝色的线代表向量w，红色的先代表直线y。我们可以看到向量w和直线的关系为垂直关系。这说明了向量w也控制这直线的方向，只不过是与这个直线的方向是垂直的。标量γ的作用也没有变，依然决定了直线的截距。此时，我们称w为直线的法向量。</p>
<p>二维空间的直线方程已经推导完成，将其推广到n为空间，就变成了超平面方程。(一个超平面，在二维空间的例子就是一个直线)但是它的公式没变，依然是：</p>
<p>$$<br>w^Tx+\gamma=0<br>$$<br>不同之处在于：</p>
<p>$$<br>w=\left[<br>\matrix{<br>  w_1,w_2,\cdots,w_n\<br>}<br>\right]^T,x=\left[<br>\matrix{<br>  x_1&amp;x_2,\cdots,x_n\<br>}<br>\right]^T<br>$$</p>
<p>我们已经顺利推导出了”决策面”方程，它就是我们的超平面方程，之后，我们统称其为超平面方程。</p>
<h3 id="（2）”分类间隔”方程"><a href="#（2）”分类间隔”方程" class="headerlink" title="（2）”分类间隔”方程"></a>（2）”分类间隔”方程</h3><p>现在，我们依然对于一个二维平面的简单例子进行推导。</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923171540791"></p>
<p>我们已经知道间隔的大小实际上就是支持向量对应的样本点到决策面的距离的二倍。那么图中的距离d我们怎么求？我们高中都学过，点到直线的距离距离公式如下：</p>
<p>$$<br>d=|\frac{Ax_0+By_0+C}{\sqrt{A^2+B^2}}|<br>$$</p>
<p>公式中的直线方程为Ax0+By0+C=0，点P的坐标为(x0,y0)。</p>
<p>现在，将直线方程扩展到多维，求得我们现在的超平面方程，对公式进行如下变形：</p>
<p>$$<br>d=\frac{|w^Tx+\gamma|}{||w||}<br>$$</p>
<p>这个d就是”分类间隔”。其中||w||表示w的二范数，求所有元素的平方和，然后再开方。比如对于二维平面：<br>$$<br>w=\left[<br>\matrix{<br>  w_1,w_2\<br>}<br>\right]^T<br>$$</p>
<p>那么，</p>
<p>$$<br>||w||=\sqrt{w_1^2+w_2^2}<br>$$</p>
<p>我们目的是为了找出一个分类效果好的超平面作为分类器。分类器的好坏的评定依据是分类间隔W=2d的大小，即分类间隔W越大，我们认为这个超平面的分类效果越好。此时，求解超平面的问题就变成了求解分类间隔W最大化的为题。W的最大化也就是d最大化的。</p>
<h3 id="（3）约束条件"><a href="#（3）约束条件" class="headerlink" title="（3）约束条件"></a>（3）约束条件</h3><p>看起来，我们已经顺利获得了目标函数的数学形式。但是为了求解w的最大值。我们不得不面对如下问题：</p>
<ul>
<li>我们如何判断超平面是否将样本点正确分类？</li>
<li>我们知道相求距离d的最大值，我们首先需要找到支持向量上的点，怎么在众多的点中选出支持向量上的点呢？</li>
</ul>
<p>上述我们需要面对的问题就是约束条件，也就是说我们优化的变量d的取值范围受到了限制和约束。事实上约束条件一直是最优化问题里最让人头疼的东西。但既然我们已经知道了这些约束条件确实存在，就不得不用数学语言对他们进行描述。但SVM算法通过一些巧妙的小技巧，将这些约束条件融合到一个不等式里面。</p>
<p>这个二维平面上有两种点，我们分别对它们进行标记：</p>
<ul>
<li>红颜色的圆点标记为1，我们人为规定其为正样本；</li>
<li>蓝颜色的五角星标记为-1，我们人为规定其为负样本。</li>
</ul>
<p>对每个样本点xi加上一个类别标签yi：</p>
<p>$$<br>y_i=\begin{cases}+1，红色点\ -1， 蓝色点\end{cases}<br>$$</p>
<p>如果我们的超平面方程能够完全正确地对上图的样本点进行分类，就会满足下面的方程：</p>
<p>$$<br>\begin{cases}w^Tx_i+\gamma&gt;0,y_i=1\ w^Tx_i+\gamma&lt;0,y_i=-1，\end{cases}<br>$$</p>
<p>如果我们要求再高一点，假设决策面正好处于间隔区域的中轴线上，并且相应的支持向量对应的样本点到决策面的距离为d，那么公式进一步写成：</p>
<p>$$<br>\begin{cases}\frac{|w^Tx+\gamma|}{||w||}\geq d ,\forall y_i=1\\frac{|w^Tx+\gamma|}{||w||}\leq -d ,\forall y_i=-1 \end{cases}<br>$$</p>
<p>上述公式的解释就是，对于所有分类标签为1的样本点，它们到直线的距离都大于等于d(支持向量上的样本点到超平面的距离)。对于所有分类标签为-1的样本点，它们到直线的距离都小于等于d。公式两边都除以d，就可以得到：</p>
<p>$$<br>\begin{cases}w^T_dx_i+\gamma_d\geq 1,\forall y_i=1\ w^T_dx_i+\gamma_d \leq-1,\forall y_i=-1，\end{cases}<br>$$</p>
<p>其中，<br>$$<br>w_d=\frac{w}{||w||d}<br>$$</p>
<p>$$<br>\gamma_d=\frac{\gamma}{||w||d}<br>$$</p>
<p>因为||w||和d都是标量。所上述公式的两个矢量，依然描述一条直线的法向量和截距。</p>
<p>$$<br>w^T_dx_i+\gamma_d= 0<br>$$</p>
<p>$$<br>w^Tx+\gamma= 0<br>$$</p>
<p>上述两个公式，都是描述一条直线，数学模型代表的意义是一样的。现在，让我们对wd和γd重新起个名字，就叫它们w和γ。因此，我们就可以说：“对于存在分类间隔的两类样本点，我们一定可以找到一些超平面面，使其对于所有的样本点均满足下面的条件：”</p>
<p>$$<br>\begin{cases}w^Tx_i+\gamma\geq 1,\forall y_i=1\ w^Tx_i+\gamma \leq-1,\forall y_i=-1，\end{cases}<br>$$</p>
<p>上述方程即给出了SVM最优化问题的约束条件。这时候，可能有人会问了，为什么标记为1和-1呢？因为这样标记方便我们将上述方程变成如下形式：</p>
<p>$$<br>y_i(w^Tx_i+\gamma)\geq1 \quad \forall x_i<br>$$</p>
<p>正是因为标签为1和-1，才方便我们将约束条件变成一个约束方程，从而方便我们的计算。</p>
<h3 id="（4）线性SVM优化问题基本描述"><a href="#（4）线性SVM优化问题基本描述" class="headerlink" title="（4）线性SVM优化问题基本描述"></a>（4）线性SVM优化问题基本描述</h3><p>现在整合一下思路，我们已经得到我们的目标函数：</p>
<p>$$<br>d=\frac{|w^Tx+\gamma|}{||w||}<br>$$</p>
<p>我们的优化目标是是d最大化。我们已经说过，我们是用支持向量上的样本点求解d的最大化的问题的。那么支持向量上的样本点有什么特点呢？</p>
<p>$$<br>|w^Tx_i+\gamma|=1 \quad \forall支持向量上的样本点x_i<br>$$</p>
<p>现在我们就可以将我们的目标函数进一步化简：<br>$$<br>d=\frac{1}{||w||}<br>$$</p>
<p>因为，我们只关心支持向量上的点。随后我们求解d的最大化问题变成了||w||的最小化问题。进而||w||的最小化问题等效于</p>
<p>$$<br>min\frac{1}{2}||w||^2<br>$$</p>
<p>为什么要做这样的等效呢？这是为了在进行最优化的过程中对目标函数求导时比较方便，但这绝对不影响最优化问题最后的求解。我们将最终的目标函数和约束条件放在一起进行描述：</p>
<p>$$<br>min\frac{1}{2}||w||^2<br>$$</p>
<p>$$<br>s.t. \quad y_i(w^Tx_i+b)\geq 1,i=1,2,\cdots,n<br>$$</p>
<p>这里n是样本点的总个数，缩写s.t.表示”Subject to”，是”服从某某条件”的意思。上述公式描述的是一个典型的不等式约束条件下的二次型函数优化问题，同时也是支持向量机的基本数学模型。</p>
<h3 id="（5）求解准备"><a href="#（5）求解准备" class="headerlink" title="（5）求解准备"></a>（5）求解准备</h3><p>我们已经得到支持向量机的基本数学模型，接下来的问题就是如何根据数学模型，求得我们想要的最优解。在学习求解方法之前，我们得知道一点，想用我下面讲述的求解方法有一个前提，就是我们的目标函数必须是凸函数。理解凸函数，我们还要先明确另一个概念，凸集。在凸几何中，凸集(convex set)是在)凸组合下闭合的放射空间的子集。看一幅图可能更容易理解：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923172757190"></p>
<p>左右量图都是一个集合。<strong>如果集合中任意2个元素连线上的点也在集合中，那么这个集合就是凸集。</strong>显然，上图中的左图是一个凸集，上图中的右图是一个非凸集。</p>
<p>凸函数的定义也是如此，其几何意义表示为函数任意两点连线上的值大于对应自变量处的函数值。若这里凸集C即某个区间L，那么，设函数f为定义在区间L上的函数，若对L上的任意两点x1，x2和任意的实数λ，λ属于(0,1)，总有：</p>
<p>$$<br>f(\lambda x_1+(1-\lambda)x_2)\leq \lambda f(x_1)+(1-\lambda)f(x_2)<br>$$</p>
<p>则函数f称为L上的凸函数，当且仅当其上镜图（在函数图像上方的点集）为一个凸集。再看一幅图，也许更容易理解：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/ml_8_43_modify.png"></p>
<p>像上图这样的函数，它整体就是一个非凸函数，我们无法获得全局最优解的，只能获得局部最优解。比如红框内的部分，如果单独拿出来，它就是一个凸函数。对于我们的目标函数：</p>
<p>$$<br>min\frac{1}{2}||w||^2<br>$$<br>很显然，它是一个凸函数。所以，可以使用我接下来讲述的方法求取最优解。</p>
<p>对于不等式的优化问题，常常使用的方法就是KKT条件。同样地，我们把所有的等式、不等式约束与f(x)写为一个式子，也叫拉格朗日函数，系数也称拉格朗日乘子，通过一些条件，可以求出最优值的<strong>必要条件</strong>，这个条件称为KKT条件。</p>
<p>必要条件和充要条件如果不理解，可以看下面这句话：</p>
<ul>
<li>A的<strong>必要条件</strong>就是A可以推出的<strong>结论</strong></li>
<li>A的<strong>充分条件</strong>就是可以推出A的<strong>前提</strong></li>
</ul>
<p>了解到这些，现在让我们再看一下我们的最优化问题：</p>
<p>$$<br>min\frac{1}{2}||w||^2<br>$$</p>
<p>$$<br>s.t. \quad y_i(w^Tx_i+b)\geq 1,i=1,2,\cdots,n<br>$$</p>
<p>在学习求解最优化问题之前，我们还要学习两个东西：拉格朗日函数和KKT条件。</p>
<h3 id="（6）拉格朗日函数"><a href="#（6）拉格朗日函数" class="headerlink" title="（6）拉格朗日函数"></a>（6）拉格朗日函数</h3><p>首先，我们先要从宏观的视野上了解一下<strong>拉格朗日对偶问题出现的原因和背景。</strong></p>
<p>我们知道我们要求解的是最小化问题，所以一个直观的想法是如果我能够构造一个函数，使得该函数在可行解区域内与原目标函数完全一致，而在可行解区域外的数值非常大，甚至是无穷大，那么这个<strong>没有约束条件的新目标函数的优化问题就</strong>与原来<strong>有约束条件的原始目标函数的优化问题</strong>是等价的问题。这就是使用拉格朗日方程的目的，它将约束条件放到目标函数中，<strong>从而将有约束优化问题转换为无约束优化问题。</strong></p>
<p>随后，人们又发现，使用拉格朗日获得的函数，使用求导的方法求解依然困难。进而，需要对问题再进行一次转换，即使用一个数学技巧：<strong>拉格朗日对偶。</strong></p>
<p>所以，显而易见的是，我们在拉格朗日优化我们的问题这个道路上，<strong>需要进行下面二个步骤：</strong></p>
<ul>
<li>将有约束的原始目标函数转换为无约束的新构造的拉格朗日目标函数</li>
<li>使用拉格朗日对偶性，将不易求解的优化问题转化为易求解的优化</li>
</ul>
<p>下面，进行第一步：<strong>将有约束的原始目标函数转换为无约束的新构造的拉格朗日目标函数</strong></p>
<p>公式变形如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923173537336"></p>
<p>其中αi是拉格朗日乘子，αi大于等于0，是我们构造新目标函数时引入的系数变量(我们自己设置)。现在我们令：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923173612138"></p>
<p>当样本点不满足约束条件时，即<strong>在可行解区域外</strong>：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923173641347"></p>
<p>此时，我们将αi设置为正无穷，此时θ(w)显然也是正无穷。</p>
<p>当样本点满足约束条件时，即<strong>在可行解区域内：</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923173710505"></p>
<p>此时，显然θ(w)为原目标函数本身。我们将上述两种情况结合一下，就得到了新的目标函数：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923173733435"></p>
<p>此时，再看我们的初衷，就是为了建立一个在可行解区域内与原目标函数相同，在可行解区域外函数值趋近于无穷大的新函数，现在我们做到了。</p>
<p>现在，我们的问题变成了求新目标函数的最小值，即：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923173800720"></p>
<p>这里用p*表示这个问题的最优值，且和最初的问题是等价的。</p>
<p>接下来，我们进行第二步：<strong>将不易求解的优化问题转化为易求解的优化</strong></p>
<p>我们看一下我们的新目标函数，先求最大值，再求最小值。这样的话，我们首先就要面对带有需要求解的参数w和b的方程，而αi又是不等式约束，这个求解过程不好做。所以，我们需要使用拉格朗日函数对偶性，将最小和最大的位置交换一下，这样就变成了：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923173832514"></p>
<p>交换以后的新问题是原始问题的对偶问题，这个新问题的最优值用d<em>来表示。而且d</em>&lt;=p*。我们关心的是d=p的时候，这才是我们要的解。需要什么条件才能让d=p呢？</p>
<ul>
<li>首先必须满足这个优化问题是凸优化问题。</li>
<li>其次，需要满足KKT条件。</li>
</ul>
<p>凸优化问题的定义是：<strong>求取最小值的目标函数为凸函数的一类优化问题。</strong>目标函数是凸函数我们已经知道，这个优化问题又是求最小值。所以我们的最优化问题就是凸优化问题。</p>
<p>接下里，就是探讨是否满足KKT条件了。</p>
<h3 id="（7）KKT条件"><a href="#（7）KKT条件" class="headerlink" title="（7）KKT条件"></a>（7）KKT条件</h3><p>我们已经使用拉格朗日函数对我们的目标函数进行了处理，生成了一个新的目标函数。通过一些条件，可以求出最优值的必要条件，这个条件就是接下来要说的KKT条件。一个最优化模型能够表示成下列标准形式：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923174024269"></p>
<p>KKT条件的全称是Karush-Kuhn-Tucker条件，KKT条件是说最优值条件必须满足以下条件：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20201231192647.png"></p>
<p>求取这三个等式之后就能得到候选最优值。其中第三个式子非常有趣，因为g(x)&lt;=0，如果要满足这个等式，必须α=0或者g(x)=0. 这是SVM的很多重要性质的来源，如支持向量的概念。KTT条件的证明不是重点先略去……</p>
<h3 id="（8）对偶问题求解"><a href="#（8）对偶问题求解" class="headerlink" title="（8）对偶问题求解"></a>（8）对偶问题求解</h3><p><strong>第一步：</strong></p>
<p>根据上述推导已知：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923174148322"></p>
<p>首先固定α，要让L(w,b,α)关于w和b最小化，我们分别对w和b偏导数，令其等于0，即：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923174216260"></p>
<p>将上述结果带回L(w,b,α)得到：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923174244541"></p>
<p>从上面的最后一个式子，我们可以看出，此时的L(w,b,α)函数只含有一个变量，即αi。</p>
<p><strong>第二步：</strong></p>
<p>现在内侧的最小值求解完成，我们求解外侧的最大值，从上面的式子得到</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923174321119"></p>
<p>现在我们的优化问题变成了如上的形式。对于这个问题，我们有更高效的优化算法，即序列最小优化（SMO）算法。我们通过这个优化算法能得到α，再根据α，我们就可以求解出w和b，进而求得我们最初的目的：找到超平面，即”决策平面”。</p>
<p><strong>总结一句话：我们为啥使出吃奶的劲儿进行推导？因为我们要将最初的原始问题，转换到可以使用SMO算法求解的问题，这是一种最流行的求解方法。为啥用这种求解方法？因为它牛逼啊！</strong></p>
<h2 id="2-SMO算法"><a href="#2-SMO算法" class="headerlink" title="2 SMO算法"></a>2 SMO算法</h2><p>现在，我们已经得到了可以用SMO算法求解的目标函数，但是对于怎么编程实现SMO算法还是感觉无从下手。那么现在就聊聊如何使用SMO算法进行求解。我们设置C为一个比较大的数。</p>
<h4 id="ai的取值范围"><a href="#ai的取值范围" class="headerlink" title="ai的取值范围"></a>ai的取值范围</h4><p>ai遵循以下条件，我们称之为g(x)目标条件</p>
<p>g(x)目标条件：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20201231192857.png"></p>
<p>除此之外还遵循初始限制条件，即我们上面推导的结果<br>$$<br>s.t.\quad \alpha_i \geq0<br>$$</p>
<p>$$<br>\Sigma^n_i \alpha_iy_i=0<br>$$</p>
<p>我们如果一次只优化一个a，那么优化后将不会满足<br>$$<br>\Sigma^n_i \alpha_iy_i=0<br>$$<br>因此我们一次优化两个a。</p>
<h4 id="参数优化考量："><a href="#参数优化考量：" class="headerlink" title="参数优化考量："></a>参数优化考量：</h4><p>经过一次优化，两个分量要有尽可能多的改变，这样才能用尽可能少的迭代优化次数让他们达到g(x)目标条件<br>$$<br>E_i=g(x_i)-y_i<br>$$</p>
<p>$$<br>|E_1-E_2|越大，优化后的、\alpha_1,\alpha_2改变越大<br>$$</p>
<h4 id="优化方法："><a href="#优化方法：" class="headerlink" title="优化方法："></a>优化方法：</h4><p>怎么优化α1、α2可以确保优化后，它们对应的样本能够满足g(x)目标条件或者违反g(x)目标条件的程度变轻呢？</p>
<p>对于我们需要优化的公式</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923174321119"></p>
<p>将α1、α2看做变量，其他分量看做常数<br>$$<br>min(\frac{1}{2}\sum_{i,j=1}^n\alpha_i\alpha_j y_iy_jx^T_jx_j-\sum_{i=1}^n\alpha_i)=m\alpha^2_1+n\alpha^2_2+k\alpha_1\alpha_2+q\alpha_1+p\alpha_2<br>$$</p>
<p>$$<br>s.t. \quad y_1\alpha_1+y_2\alpha_2=k ,\quad 0\leq\alpha_1\leq C ,\quad 0\leq\alpha_2\leq C<br>$$</p>
<p>假设<br>$$<br>y_1=1,y_2=1<br>$$<br>所以<br>$$<br>\alpha_1+\alpha_2=1,\quad \alpha_1=k-\alpha_2<br>$$<br>代入上式min（）对a2求导令其等零，得出a2_new<br>$$<br>0 \leq k-\alpha_2\leq C,0 \leq \alpha_2\leq C<br>$$<br>所以<br>$$<br>max(k-c,0)\leq \alpha_2 \leq min(k,c)<br>$$<br>若是a2_new在限制条件内，求出a1_new，若不在，截取可用范围得出a2_new_new,再求出a1_new_new</p>
<h1 id="四-编程求解线性SVM"><a href="#四-编程求解线性SVM" class="headerlink" title="四 编程求解线性SVM"></a>四 编程求解线性SVM</h1><p>已经梳理完了SMO算法实现步骤，接下来按照这个思路编写代码，进行实战练习。</p>
<h2 id="（1）可视化数据集"><a href="#（1）可视化数据集" class="headerlink" title="（1）可视化数据集"></a>（1）可视化数据集</h2><p>我们先使用简单的数据集进行测试，数据集下载地址：<a target="_blank" rel="noopener" href="https://github.com/Jack-Cherish/Machine-Learning/blob/master/SVM/testSet.txt">https://github.com/Jack-Cherish/Machine-Learning/blob/master/SVM/testSet.txt</a></p>
<p>编写程序可视化数据集，看下它是长什么样的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:读取数据</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    fileName - 文件名</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    dataMat - 数据矩阵</span></span><br><span class="line"><span class="string">    labelMat - 数据标签</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-21</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>(<span class="params">fileName</span>):</span></span><br><span class="line">    dataMat = []; labelMat = []</span><br><span class="line">    fr = <span class="built_in">open</span>(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():                                     <span class="comment">#逐行读取，滤除空格等</span></span><br><span class="line">        lineArr = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        dataMat.append([<span class="built_in">float</span>(lineArr[<span class="number">0</span>]), <span class="built_in">float</span>(lineArr[<span class="number">1</span>])])      <span class="comment">#添加数据</span></span><br><span class="line">        labelMat.append(<span class="built_in">float</span>(lineArr[<span class="number">2</span>]))                          <span class="comment">#添加标签</span></span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:数据可视化</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataMat - 数据矩阵</span></span><br><span class="line"><span class="string">    labelMat - 数据标签</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    无</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-21</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showDataSet</span>(<span class="params">dataMat, labelMat</span>):</span></span><br><span class="line">    data_plus = []                                  <span class="comment">#正样本</span></span><br><span class="line">    data_minus = []                                 <span class="comment">#负样本</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataMat)):</span><br><span class="line">        <span class="keyword">if</span> labelMat[i] &gt; <span class="number">0</span>:</span><br><span class="line">            data_plus.append(dataMat[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data_minus.append(dataMat[i])</span><br><span class="line">    data_plus_np = np.array(data_plus)              <span class="comment">#转换为numpy矩阵</span></span><br><span class="line">    data_minus_np = np.array(data_minus)            <span class="comment">#转换为numpy矩阵</span></span><br><span class="line">    plt.scatter(np.transpose(data_plus_np)[<span class="number">0</span>], np.transpose(data_plus_np)[<span class="number">1</span>])   <span class="comment">#正样本散点图</span></span><br><span class="line">    plt.scatter(np.transpose(data_minus_np)[<span class="number">0</span>], np.transpose(data_minus_np)[<span class="number">1</span>]) <span class="comment">#负样本散点图</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    dataMat, labelMat = loadDataSet(<span class="string">&#x27;testSet.txt&#x27;</span>)</span><br><span class="line">    showDataSet(dataMat, labelMat)</span><br><span class="line"><span class="number">12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364</span></span><br></pre></td></tr></table></figure>
<p>运行程序，查看结果：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923183052127"></p>
<p>这就是我们使用的二维数据集，显然线性可分。现在我们使用简化版的SMO算法进行求解。</p>
<h2 id="（2）简化版SMO算法"><a href="#（2）简化版SMO算法" class="headerlink" title="（2）简化版SMO算法"></a>（2）简化版SMO算法</h2><p>按照上述已经推导的步骤编写代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:UTF-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> types</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:读取数据</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    fileName - 文件名</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    dataMat - 数据矩阵</span></span><br><span class="line"><span class="string">    labelMat - 数据标签</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-21</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span>(<span class="params">fileName</span>):</span></span><br><span class="line">    dataMat = []; labelMat = []</span><br><span class="line">    fr = <span class="built_in">open</span>(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():                                     <span class="comment">#逐行读取，滤除空格等</span></span><br><span class="line">        lineArr = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        dataMat.append([<span class="built_in">float</span>(lineArr[<span class="number">0</span>]), <span class="built_in">float</span>(lineArr[<span class="number">1</span>])])      <span class="comment">#添加数据</span></span><br><span class="line">        labelMat.append(<span class="built_in">float</span>(lineArr[<span class="number">2</span>]))                          <span class="comment">#添加标签</span></span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:随机选择alpha</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    i - alpha</span></span><br><span class="line"><span class="string">    m - alpha参数个数</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    j -</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-21</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectJrand</span>(<span class="params">i, m</span>):</span></span><br><span class="line">    j = i                                 <span class="comment">#选择一个不等于i的j</span></span><br><span class="line">    <span class="keyword">while</span> (j == i):</span><br><span class="line">        j = <span class="built_in">int</span>(random.uniform(<span class="number">0</span>, m))</span><br><span class="line">    <span class="keyword">return</span> j</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:修剪alpha</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    aj - alpha值</span></span><br><span class="line"><span class="string">    H - alpha上限</span></span><br><span class="line"><span class="string">    L - alpha下限</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    aj - alpah值</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-21</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clipAlpha</span>(<span class="params">aj,H,L</span>):</span></span><br><span class="line">    <span class="keyword">if</span> aj &gt; H:</span><br><span class="line">        aj = H</span><br><span class="line">    <span class="keyword">if</span> L &gt; aj:</span><br><span class="line">        aj = L</span><br><span class="line">    <span class="keyword">return</span> aj</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:简化版SMO算法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataMatIn - 数据矩阵</span></span><br><span class="line"><span class="string">    classLabels - 数据标签</span></span><br><span class="line"><span class="string">    C - 松弛变量</span></span><br><span class="line"><span class="string">    toler - 容错率</span></span><br><span class="line"><span class="string">    maxIter - 最大迭代次数</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    无</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-23</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smoSimple</span>(<span class="params">dataMatIn, classLabels, C, toler, maxIter</span>):</span></span><br><span class="line">    <span class="comment">#转换为numpy的mat存储</span></span><br><span class="line">    dataMatrix = np.mat(dataMatIn); labelMat = np.mat(classLabels).transpose()</span><br><span class="line">    <span class="comment">#初始化b参数，统计dataMatrix的维度</span></span><br><span class="line">    b = <span class="number">0</span>; m,n = np.shape(dataMatrix)</span><br><span class="line">    <span class="comment">#初始化alpha参数，设为0</span></span><br><span class="line">    alphas = np.mat(np.zeros((m,<span class="number">1</span>)))</span><br><span class="line">    <span class="comment">#初始化迭代次数</span></span><br><span class="line">    iter_num = <span class="number">0</span></span><br><span class="line">    <span class="comment">#最多迭代matIter次</span></span><br><span class="line">    <span class="keyword">while</span> (iter_num &lt; maxIter):</span><br><span class="line">        alphaPairsChanged = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="comment">#步骤1：计算误差Ei</span></span><br><span class="line">            fXi = <span class="built_in">float</span>(np.multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b</span><br><span class="line">            Ei = fXi - <span class="built_in">float</span>(labelMat[i])</span><br><span class="line">            <span class="comment">#优化alpha，更设定一定的容错率。</span></span><br><span class="line">            <span class="keyword">if</span> ((labelMat[i]*Ei &lt; -toler) <span class="keyword">and</span> (alphas[i] &lt; C)) <span class="keyword">or</span> ((labelMat[i]*Ei &gt; toler) <span class="keyword">and</span> (alphas[i] &gt; <span class="number">0</span>)):</span><br><span class="line">                <span class="comment">#随机选择另一个与alpha_i成对优化的alpha_j</span></span><br><span class="line">                j = selectJrand(i,m)</span><br><span class="line">                <span class="comment">#步骤1：计算误差Ej</span></span><br><span class="line">                fXj = <span class="built_in">float</span>(np.multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b</span><br><span class="line">                Ej = fXj - <span class="built_in">float</span>(labelMat[j])</span><br><span class="line">                <span class="comment">#保存更新前的aplpha值，使用深拷贝</span></span><br><span class="line">                alphaIold = alphas[i].copy(); alphaJold = alphas[j].copy();</span><br><span class="line">                <span class="comment">#步骤2：计算上下界L和H</span></span><br><span class="line">                <span class="keyword">if</span> (labelMat[i] != labelMat[j]):</span><br><span class="line">                    L = <span class="built_in">max</span>(<span class="number">0</span>, alphas[j] - alphas[i])</span><br><span class="line">                    H = <span class="built_in">min</span>(C, C + alphas[j] - alphas[i])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    L = <span class="built_in">max</span>(<span class="number">0</span>, alphas[j] + alphas[i] - C)</span><br><span class="line">                    H = <span class="built_in">min</span>(C, alphas[j] + alphas[i])</span><br><span class="line">                <span class="keyword">if</span> L==H: print(<span class="string">&quot;L==H&quot;</span>); <span class="keyword">continue</span></span><br><span class="line">                <span class="comment">#步骤3：计算eta</span></span><br><span class="line">                eta = <span class="number">2.0</span> * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T</span><br><span class="line">                <span class="keyword">if</span> eta &gt;= <span class="number">0</span>: print(<span class="string">&quot;eta&gt;=0&quot;</span>); <span class="keyword">continue</span></span><br><span class="line">                <span class="comment">#步骤4：更新alpha_j</span></span><br><span class="line">                alphas[j] -= labelMat[j]*(Ei - Ej)/eta</span><br><span class="line">                <span class="comment">#步骤5：修剪alpha_j</span></span><br><span class="line">                alphas[j] = clipAlpha(alphas[j],H,L)</span><br><span class="line">                <span class="keyword">if</span> (<span class="built_in">abs</span>(alphas[j] - alphaJold) &lt; <span class="number">0.00001</span>): print(<span class="string">&quot;alpha_j变化太小&quot;</span>); <span class="keyword">continue</span></span><br><span class="line">                <span class="comment">#步骤6：更新alpha_i</span></span><br><span class="line">                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])</span><br><span class="line">                <span class="comment">#步骤7：更新b_1和b_2</span></span><br><span class="line">                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T</span><br><span class="line">                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T</span><br><span class="line">                <span class="comment">#步骤8：根据b_1和b_2更新b</span></span><br><span class="line">                <span class="keyword">if</span> (<span class="number">0</span> &lt; alphas[i]) <span class="keyword">and</span> (C &gt; alphas[i]): b = b1</span><br><span class="line">                <span class="keyword">elif</span> (<span class="number">0</span> &lt; alphas[j]) <span class="keyword">and</span> (C &gt; alphas[j]): b = b2</span><br><span class="line">                <span class="keyword">else</span>: b = (b1 + b2)/<span class="number">2.0</span></span><br><span class="line">                <span class="comment">#统计优化次数</span></span><br><span class="line">                alphaPairsChanged += <span class="number">1</span></span><br><span class="line">                <span class="comment">#打印统计信息</span></span><br><span class="line">                print(<span class="string">&quot;第%d次迭代 样本:%d, alpha优化次数:%d&quot;</span> % (iter_num,i,alphaPairsChanged))</span><br><span class="line">        <span class="comment">#更新迭代次数</span></span><br><span class="line">        <span class="keyword">if</span> (alphaPairsChanged == <span class="number">0</span>): iter_num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>: iter_num = <span class="number">0</span></span><br><span class="line">        print(<span class="string">&quot;迭代次数: %d&quot;</span> % iter_num)</span><br><span class="line">    <span class="keyword">return</span> b,alphas</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:分类结果可视化</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataMat - 数据矩阵</span></span><br><span class="line"><span class="string">    w - 直线法向量</span></span><br><span class="line"><span class="string">    b - 直线解决</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    无</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-23</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showClassifer</span>(<span class="params">dataMat, w, b</span>):</span></span><br><span class="line">    <span class="comment">#绘制样本点</span></span><br><span class="line">    data_plus = []                                  <span class="comment">#正样本</span></span><br><span class="line">    data_minus = []                                 <span class="comment">#负样本</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataMat)):</span><br><span class="line">        <span class="keyword">if</span> labelMat[i] &gt; <span class="number">0</span>:</span><br><span class="line">            data_plus.append(dataMat[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            data_minus.append(dataMat[i])</span><br><span class="line">    data_plus_np = np.array(data_plus)              <span class="comment">#转换为numpy矩阵</span></span><br><span class="line">    data_minus_np = np.array(data_minus)            <span class="comment">#转换为numpy矩阵</span></span><br><span class="line">    plt.scatter(np.transpose(data_plus_np)[<span class="number">0</span>], np.transpose(data_plus_np)[<span class="number">1</span>], s=<span class="number">30</span>, alpha=<span class="number">0.7</span>)   <span class="comment">#正样本散点图</span></span><br><span class="line">    plt.scatter(np.transpose(data_minus_np)[<span class="number">0</span>], np.transpose(data_minus_np)[<span class="number">1</span>], s=<span class="number">30</span>, alpha=<span class="number">0.7</span>) <span class="comment">#负样本散点图</span></span><br><span class="line">    <span class="comment">#绘制直线</span></span><br><span class="line">    x1 = <span class="built_in">max</span>(dataMat)[<span class="number">0</span>]</span><br><span class="line">    x2 = <span class="built_in">min</span>(dataMat)[<span class="number">0</span>]</span><br><span class="line">    a1, a2 = w</span><br><span class="line">    b = <span class="built_in">float</span>(b)</span><br><span class="line">    a1 = <span class="built_in">float</span>(a1[<span class="number">0</span>])</span><br><span class="line">    a2 = <span class="built_in">float</span>(a2[<span class="number">0</span>])</span><br><span class="line">    y1, y2 = (-b- a1*x1)/a2, (-b - a1*x2)/a2</span><br><span class="line">    plt.plot([x1, x2], [y1, y2])</span><br><span class="line">    <span class="comment">#找出支持向量点</span></span><br><span class="line">    <span class="keyword">for</span> i, alpha <span class="keyword">in</span> <span class="built_in">enumerate</span>(alphas):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(alpha) &gt; <span class="number">0</span>:</span><br><span class="line">            x, y = dataMat[i]</span><br><span class="line">            plt.scatter([x], [y], s=<span class="number">150</span>, c=<span class="string">&#x27;none&#x27;</span>, alpha=<span class="number">0.7</span>, linewidth=<span class="number">1.5</span>, edgecolor=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">函数说明:计算w</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Parameters:</span></span><br><span class="line"><span class="string">    dataMat - 数据矩阵</span></span><br><span class="line"><span class="string">    labelMat - 数据标签</span></span><br><span class="line"><span class="string">    alphas - alphas值</span></span><br><span class="line"><span class="string">Returns:</span></span><br><span class="line"><span class="string">    无</span></span><br><span class="line"><span class="string">Author:</span></span><br><span class="line"><span class="string">    Jack Cui</span></span><br><span class="line"><span class="string">Blog:</span></span><br><span class="line"><span class="string">    http://blog.csdn.net/c406495762</span></span><br><span class="line"><span class="string">Zhihu:</span></span><br><span class="line"><span class="string">    https://www.zhihu.com/people/Jack--Cui/</span></span><br><span class="line"><span class="string">Modify:</span></span><br><span class="line"><span class="string">    2017-09-23</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_w</span>(<span class="params">dataMat, labelMat, alphas</span>):</span></span><br><span class="line">    alphas, dataMat, labelMat = np.array(alphas), np.array(dataMat), np.array(labelMat)</span><br><span class="line">    w = np.dot((np.tile(labelMat.reshape(<span class="number">1</span>, -<span class="number">1</span>).T, (<span class="number">1</span>, <span class="number">2</span>)) * dataMat).T, alphas)</span><br><span class="line">    <span class="keyword">return</span> w.tolist()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    dataMat, labelMat = loadDataSet(<span class="string">&#x27;testSet.txt&#x27;</span>)</span><br><span class="line">    b,alphas = smoSimple(dataMat, labelMat, <span class="number">0.6</span>, <span class="number">0.001</span>, <span class="number">40</span>)</span><br><span class="line">    w = get_w(dataMat, labelMat, alphas)</span><br><span class="line">    showClassifer(dataMat, w, b)</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239</span></span><br></pre></td></tr></table></figure>
<p>程序运行结果：</p>
<p><img src="https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img03/20170923183337571"></p>
<p>其中，中间的蓝线为求出来的分类器，用红圈圈出的点为支持向量点。</p>
<h1 id="五-总结"><a href="#五-总结" class="headerlink" title="五 总结"></a>五 总结</h1><ul>
<li>本文主要进行了线性SVM的推导，并通过编程实现一个简化版SMO算法；</li>
<li>本文的简化版SMO算法在选取α的时候，没有选择启发式的选择方法，并且没有两个乘子的计算没有进行优化，所以算法比较耗时，下一篇文章会讲解相应的优化方法；</li>
<li>本文讨论的是线性SVM，没有使用核函数，下一篇文章将会讲解如何应用核函数，将SVM应用于非线性数据集；</li>
<li>如有问题，请留言。如有错误，还望指正，谢谢！</li>
</ul>
<p><strong>PS： 如果觉得本篇本章对您有所帮助，欢迎关注、评论、赞！</strong></p>
<p><strong>参考资料：</strong></p>
<ul>
<li>[1] 五岁小孩也能看懂的SVM：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/21094489/answer/8627319">https://www.zhihu.com/question/21094489/answer/8627319</a></li>
<li>[2] 五岁小孩也能看懂的SVM ：<a target="_blank" rel="noopener" href="https://www.reddit.com/r/MachineLearning/comments/15zrpp/please_explain_support_vector_machines_svm_like_i/">https://www.reddit.com/r/MachineLearning/comments/15zrpp/please_explain_support_vector_machines_svm_like_i/</a></li>
<li>[3] pluskid大牛博客：<a target="_blank" rel="noopener" href="http://blog.pluskid.org/?page_id=683">http://blog.pluskid.org/?page_id=683</a></li>
<li>[4] 陈东岳老师文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/24638007">https://zhuanlan.zhihu.com/p/24638007</a></li>
<li>[5] 深入理解拉格朗日乘子法和KKT条件：<a target="_blank" rel="noopener" href="http://blog.csdn.net/xianlingmao/article/details/7919597">http://blog.csdn.net/xianlingmao/article/details/7919597</a></li>
<li>[6] 充分条件和必要条件：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/30469121">https://www.zhihu.com/question/30469121</a></li>
<li>[7] 凸函数：<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%87%B8%E5%87%BD%E6%95%B0">https://zh.wikipedia.org/wiki/%E5%87%B8%E5%87%BD%E6%95%B0</a></li>
<li>[8]《机器学习实战》第6章内容。</li>
<li>[9] SVM之SMO算法：<a target="_blank" rel="noopener" href="http://www.cnblogs.com/zangrunqiang/p/5515872.html">http://www.cnblogs.com/zangrunqiang/p/5515872.html</a></li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">xiaobubuya</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://xiaobubuya.github.io/SVM/">https://xiaobubuya.github.io/SVM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/SVM/">SVM</a></div><div class="social-share pull-right" data-disabled="facebook"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/ajax%E5%88%9D%E4%BD%93%E9%AA%8C/"><i class="fa fa-chevron-left">  </i><span>ajax初体验</span></a></div><div class="next-post pull-right"><a href="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/"><span>计算机网络面试知识点总结</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://cdn.jsdelivr.net/gh/xiaobubuya/image@master/img01/%E6%B1%9F%E5%8D%97%E7%83%A7%E9%85%924k%E5%8A%A8%E6%BC%AB%E5%A3%81%E7%BA%B8_%E5%BD%BC%E5%B2%B8%E5%9B%BE%E7%BD%91.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2021 By xiaobubuya</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>